{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kratosgado/audio-steganography/blob/staging/steg-ai/Final_Implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "QXp8Vn1BAu2P"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install stable-baselines3 shimmy torchvggish"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "iZE9vuGCIj4l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "400a5091-f769-4127-d664-043948e15c8b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "partially initialized module 'torch' has no attribute 'fx' (most likely due to a circular import)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2503010370.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfft\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   2262\u001b[0m \u001b[0;31m# quantization depends on torch.fx and torch.ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2263\u001b[0m \u001b[0;31m# Import quantization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2264\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mquantization\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mquantization\u001b[0m  \u001b[0;31m# usort: skip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2266\u001b[0m \u001b[0;31m# Import the quasi random sampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/quantization/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfuser_method_mappings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mobserver\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mqconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mquant_type\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mquantization_mappings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/quantization/qconfig.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mhere\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \"\"\"\n\u001b[0;32m----> 9\u001b[0;31m from torch.ao.quantization.qconfig import (\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0m_add_module_to_qconfig_obs_ctr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0m_assert_valid_qconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfuser_method_mappings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mobserver\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m from .pt2e._numeric_debugger import (  # noqa: F401\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mcompare_results\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mCUSTOM_KEY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/pt2e/_numeric_debugger.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mao\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompute_sqnr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mao\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpt2e\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbfs_trace_with_node_process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExportedProgram\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGraphModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/pt2e/graph_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExportedProgram\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m from torch.fx.passes.utils.source_matcher_utils import (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/export/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pytree\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpytree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compatibility\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpass_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPassResult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpass_manager\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPassManager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFileLike\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/fx/passes/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m from . import (\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mgraph_drawer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mgraph_manipulation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mnet_min_base\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0moperator_support\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/fx/passes/runtime_assert.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_subclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_sparse_any\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compatibility\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlazy_format_graph_code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproxy_tensor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpy_sym_types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msym_node\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSymNode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/fx/_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mfirst_call_function_nn_module_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \"\"\"\n\u001b[1;32m     48\u001b[0m     \u001b[0mReturns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mnn_module_stack\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfirst\u001b[0m \u001b[0mcall_function\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'torch' has no attribute 'fx' (most likely due to a circular import)"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "import torch.nn as nn\n",
        "from scipy.fft import dct, idct\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.distributions import Normal\n",
        "import torch.nn.functional as F\n",
        "import gymnasium as gym\n",
        "import random\n",
        "from gymnasium import spaces\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
        "from stable_baselines3.common.callbacks import BaseCallback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5uq1C49Mc3w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fc63ceb-b251-497b-d076-0236304afd6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# warnings.filterwarnings(\"ignore\")\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNC_gfrXNLFw"
      },
      "source": [
        "# --- UTILITY FUNCTIONS ---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_55FxjzNNKHD"
      },
      "outputs": [],
      "source": [
        "NORM_NUM = 32768\n",
        "def string_to_bits(message):\n",
        "  \"\"\"Convert a string to a sequence of bits.\"\"\"\n",
        "  return np.array([int(bit) for bit in ''.join(format(ord(char), '08b') for char in message)], dtype=np.float32)\n",
        "\n",
        "def bits_to_string(bits):\n",
        "  \"\"\"Convert a sequence of bits to a string.\"\"\"\n",
        "  text = ''\n",
        "  for bit in range(0, len(bits), 8):\n",
        "    if bit + 8 <= len(bits):\n",
        "      byte = ''.join(str(bit) for bit in bits[bit:bit + 8])\n",
        "      text += chr(int(byte, 2))\n",
        "  return text\n",
        "\n",
        "class CustomLoggingCallback(BaseCallback):\n",
        "  \"\"\" A custom callback that logs additional information from the environment. \"\"\"\n",
        "  def __init__(self, verbose=0):\n",
        "    super(CustomLoggingCallback, self).__init__(verbose)\n",
        "\n",
        "  def _on_step(self) -> bool:\n",
        "    \"\"\"This method is called after each step in the environment.   \"\"\"\n",
        "    # Accessing environment infos from the VecEnv wrapper\n",
        "    if self.locals.get('infos'):\n",
        "      for info in self.locals['infos']:\n",
        "        # Ensure info is a dictionary and contains the required keys\n",
        "        if isinstance(info, dict):\n",
        "          self.logger.record('rollout/ep_snr', info['snr'])\n",
        "          self.logger.record('rollout/ep_reward', info['reward'])\n",
        "          # self.logger.record('rollout/ep_snr', info['psnr'])\n",
        "          self.logger.record('rollout/ep_ber', info['ber'])\n",
        "          self.logger.record('rollout/ep_extraction_accuracy', info['extraction_accuracy'])\n",
        "          # self.logger.record('rollout/ep_detection_prob', info['detection_prob'])\n",
        "          self.logger.record('rollout/ep_action', info['action'])\n",
        "    return True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HV_OdAlXNoLW"
      },
      "source": [
        "# --- HYPERPARAMETERS ---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLavUsivNtBI"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "  \"\"\"Class to hold all hyperparameters.\"\"\"\n",
        "  #Audio processing\n",
        "  FRAME_SIZE = 1024 #2048\n",
        "  HOP_LENGTH = 256 #512\n",
        "  NON_CRITICAL_PERCENT = 0.1 # Modify bottom 10% of coeffs by magnitude\n",
        "  STATE_DIM = 1024\n",
        "  SAMPLE_RATE = 22050\n",
        "  N_MELS = 128\n",
        "\n",
        "  # RL Training\n",
        "  EPISODES = 60000\n",
        "  # EPISODES = 100000\n",
        "  LEARNING_RATE_ACTOR = 3e-4\n",
        "  LEARNING_RATE_CRITIC = 3e-4\n",
        "  GAMMA = 0.99 # Discount factor\n",
        "  GAE_LAMBDA = 0.95 # Lambda for Generalized Advantage Estimation\n",
        "  PPO_EPSILON = 0.2 # Epsilon for clipping in PPO\n",
        "  PPO_EPOCHS = 10 # Number of epochs for PPO update\n",
        "  N_STEPS = 256\n",
        "  CLIP_RANGE=0.2\n",
        "  ENT_COEF=0.01\n",
        "  BATCH_SIZE = 64\n",
        "  LEARNING_RATE = 3e-4\n",
        "\n",
        "  # Environment Network Pre-training\n",
        "  PRETRAIN_EPOCHS = 10\n",
        "  PRE_TRAIN_LR = 1e-3\n",
        "  PRE_TRAIN_BATCH_SIZE = 32\n",
        "\n",
        "  # Reward weights\n",
        "  SNR_WEIGHTS = 0.4 # Weight for SNR in reward. Tuned to be on a similar scale to detection prob.\n",
        "  # DETECTION_WEIGHT = 1.0 # Weight for detection probability in reward.\n",
        "  # IMPERCEPTIBILITY_WEIGHT = 0.3\n",
        "  # UNDETECTABILITY_WEIGHT = 0.3\n",
        "  EXTRACTION_ACCURACY_WEIGHT = 0.6\n",
        "\n",
        "  # spread spectrum parameters\n",
        "  CARRIER_FREQ_SIZE = 16\n",
        "  CHIP_RATE_SIZE = 8\n",
        "  SNR_DB_SIZE = 8\n",
        "  MSG_SIZE = 16\n",
        "  TOTAL_PARAM_SIZE = CARRIER_FREQ_SIZE + CHIP_RATE_SIZE + SNR_DB_SIZE + MSG_SIZE\n",
        "cfg = Config()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-feRKGXJB5t"
      },
      "source": [
        "# --- MODULE 1: AUDIO PREPROCESSOR ---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-rBWB4UJAyM"
      },
      "outputs": [],
      "source": [
        "class AudioPreprocessor:\n",
        "    \"\"\"Handles audio loading, MDCT, and inverse MDCT with sign preservation.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def load_audio(path):\n",
        "        \"\"\"Load WAV audio file and resample to cfg.SAMPLE_RATE\"\"\"\n",
        "        audio, _ = librosa.load(path, sr=cfg.SAMPLE_RATE)\n",
        "        return AudioPreprocessor.normalize_audio(audio)\n",
        "\n",
        "    @staticmethod\n",
        "    def resample_audio(waveform, sr) -> np.ndarray:\n",
        "        \"\"\"Resample audio to cfg.SAMPLE_RATE\"\"\"\n",
        "        audio = librosa.resample(waveform, orig_sr=sr, target_sr=cfg.SAMPLE_RATE)\n",
        "        return AudioPreprocessor.normalize_audio(audio)\n",
        "\n",
        "    @staticmethod\n",
        "    def normalize_audio(waveform) -> np.ndarray:\n",
        "        \"\"\"Normalize audio to [-1, 1]\"\"\"\n",
        "        return waveform / np.max(np.abs(waveform))\n",
        "\n",
        "    @staticmethod\n",
        "    def stft(waveform: np.ndarray):\n",
        "        \"\"\"Compute Short-Time Fourier Transform (STFT)\"\"\"\n",
        "        return librosa.stft(waveform, n_fft=cfg.FRAME_SIZE, hop_length=cfg.HOP_LENGTH)\n",
        "\n",
        "    @staticmethod\n",
        "    def istft(stft_matrix, length):\n",
        "        \"\"\"Compute Inverse Short-Time Fourier Transform (ISTFT)\"\"\"\n",
        "        return librosa.istft(stft_matrix, hop_length=cfg.HOP_LENGTH, n_fft=cfg.FRAME_SIZE, length=length)\n",
        "\n",
        "    @staticmethod\n",
        "    def compute_mdct(waveform):\n",
        "        \"\"\"\n",
        "        Compute Modified Discrete Cosine Transform (MDCT) using STFT and DCT.\n",
        "        Returns both magnitudes and phases to preserve sign information.\n",
        "        \"\"\"\n",
        "        stft = AudioPreprocessor.stft(waveform)\n",
        "        magnitudes = np.abs(stft)\n",
        "        phases = np.angle(stft)\n",
        "        return magnitudes, phases\n",
        "\n",
        "    @staticmethod\n",
        "    def get_non_critical_coeffs(mdct_coeffs, percentile=10):\n",
        "        \"\"\"Identify non-critical coefficients (lowest magnitude)\"\"\"\n",
        "        magnitudes = np.abs(mdct_coeffs)\n",
        "        threshold = np.percentile(magnitudes.flatten(), percentile)\n",
        "        return magnitudes < threshold\n",
        "\n",
        "    @staticmethod\n",
        "    def reconstruct_audio(magnitudes, phases, length):\n",
        "        \"\"\"\n",
        "        Reconstruct audio from magnitude and phase.\n",
        "        The phase information preserves the signs even if magnitudes are modified.\n",
        "        \"\"\"\n",
        "        # Reconstruct complex STFT\n",
        "        stft = magnitudes * np.exp(1j * phases)\n",
        "\n",
        "        # Inverse STFT to get audio\n",
        "        reconstructed_audio = AudioPreprocessor.istft(stft, length)\n",
        "        return reconstructed_audio\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_spectrogram(waveform, sr, title):\n",
        "        \"\"\"Visualize audio spectrogram\"\"\"\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        S = librosa.amplitude_to_db(np.abs(librosa.stft(waveform)), ref=np.max)\n",
        "        librosa.display.specshow(S, sr=sr, x_axis='time', y_axis='log')\n",
        "        plt.colorbar(format='%+2.0f dB')\n",
        "        plt.title(title)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_mdct_coefficients(mdct_coeffs, title=\"MDCT Coefficients\"):\n",
        "        \"\"\"Visualize MDCT coefficients\"\"\"\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.imshow(np.abs(mdct_coeffs), aspect='auto', origin='lower')\n",
        "        plt.colorbar()\n",
        "        plt.title(f\"{title} - Magnitudes\")\n",
        "        plt.xlabel(\"Time Frame\")\n",
        "        plt.ylabel(\"Frequency Bin\")\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.imshow(np.sign(mdct_coeffs), aspect='auto', origin='lower', cmap='RdBu')\n",
        "        plt.colorbar()\n",
        "        plt.title(f\"{title} - Signs\")\n",
        "        plt.xlabel(\"Time Frame\")\n",
        "        plt.ylabel(\"Frequency Bin\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    @staticmethod\n",
        "    def save_audio(audio: np.ndarray, sr: int, path: str):\n",
        "        \"\"\"Save audio to file\"\"\"\n",
        "        sf.write(path, audio, sr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyIIZSInKMwW"
      },
      "source": [
        "# --- MODULE 2: EMBEDDING/EXTRACTION MODULE ---\n",
        "Main Sign encoding class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bc5xRFmqKK_m"
      },
      "outputs": [],
      "source": [
        "from abc import ABC, abstractmethod\n",
        "\n",
        "class EmbeddingModule(ABC):\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  @abstractmethod\n",
        "  def embed(self, *args, **kwargs) -> np.ndarray:\n",
        "    pass\n",
        "\n",
        "  @abstractmethod\n",
        "  def extract(self, *args, **kwargs) -> list[int]:\n",
        "    pass\n",
        "\n",
        "  @abstractmethod\n",
        "  def set_parameters(self, action):\n",
        "    pass\n",
        "\n",
        "class SignEncoding(EmbeddingModule):\n",
        "  \"\"\"Embeds and extracts messages from MDCT coefficients.\"\"\"\n",
        "\n",
        "  def set_parameters(self, action):\n",
        "    return action[0]\n",
        "\n",
        "  def embed(self, waveform, action, msg_bits: np.ndarray, **kwargs):\n",
        "    \"\"\"Embed message using sign encoding in non-critical coefficients\"\"\"\n",
        "    magnitudes, phases = AudioPreprocessor.compute_mdct(waveform)\n",
        "    self.mask = AudioPreprocessor.get_non_critical_coeffs(magnitudes)\n",
        "    alpha = action\n",
        "    # Apply mask to get non-critical coefficients\n",
        "    coeffs = magnitudes.copy()\n",
        "    non_critical = coeffs[self.mask]\n",
        "\n",
        "    # Ensure we have enough coefficients for the message\n",
        "    if len(non_critical) < len(msg_bits):\n",
        "        raise ValueError(\"Message too long for available non-critical coefficients\")\n",
        "\n",
        "    # Embed message using sign encoding\n",
        "    for i, bit in enumerate(msg_bits):\n",
        "        sign = 1 if bit == 1 else -1\n",
        "        non_critical[i] = sign * np.abs(non_critical[i]) * (1 + alpha)\n",
        "\n",
        "    # Update coefficients\n",
        "    coeffs[self.mask] = non_critical\n",
        "    self.magnitudes = coeffs\n",
        "\n",
        "\n",
        "    return AudioPreprocessor.reconstruct_audio(coeffs, phases, len(waveform))\n",
        "\n",
        "  def extract(self, stego_waveform, bits_len, **kwargs):\n",
        "    \"\"\"Extract message from non-critical coefficients\"\"\"\n",
        "    # Apply mask to get non-critical coefficients\n",
        "    # magnitudes, _ = AudioPreprocessor.compute_mdct(stego_waveform)\n",
        "    non_critical = self.magnitudes[self.mask]\n",
        "\n",
        "    # Extract message from sign\n",
        "    msg_bits = []\n",
        "    for i in range(bits_len):\n",
        "        sign = 1 if non_critical[i] >= 0 else -1\n",
        "        bit = 1 if sign > 0 else 0\n",
        "        msg_bits.append(bit)\n",
        "\n",
        "    # Convert binary to string\n",
        "    return msg_bits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmFr72llR9ks"
      },
      "source": [
        "### Spread Spectrum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iv0zqtkMSArs"
      },
      "outputs": [],
      "source": [
        "class SpreadSpectrum(EmbeddingModule):\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize the Spread Spectrum steganography system\"\"\"\n",
        "        # Parameters for Gold sequence generation (example values)\n",
        "        self.taps1 = [5, 2]\n",
        "        self.taps2 = [5, 4, 2, 1]  # Must be a preferred pair with taps1\n",
        "        self.seed1 = 0b11111\n",
        "        self.seed2 = 0b10101\n",
        "\n",
        "        self.action_ranges = {\n",
        "            \"carrier_freq\": (5000, 15000),\n",
        "            \"chip_rate\": (3, 200),\n",
        "            \"snr\": (20, 150),\n",
        "        }\n",
        "\n",
        "    def _scale_action(self, normalized_val, low, high):\n",
        "        \"\"\"Scale from [-1, 1] to [low, high]\"\"\"\n",
        "        return int(low + (normalized_val) * (high - low))\n",
        "\n",
        "    def set_parameters(self, action):\n",
        "        \"\"\"\n",
        "        Parameters(action):\n",
        "          carrier_freq (int): Carrier frequency in Hz for embedding\n",
        "          chip_rate (int): How many samples per bit (spreading factor)\n",
        "          snr (int): Desired Signal to noise ratio in dB for embedding\n",
        "        \"\"\"\n",
        "        # Scale normalized actions to original ranges\n",
        "        carrier_freq = self._scale_action(\n",
        "            action[0], *self.action_ranges[\"carrier_freq\"]\n",
        "        )\n",
        "        chip_rate = self._scale_action(action[1], *self.action_ranges[\"chip_rate\"])\n",
        "        snr_db = self._scale_action(action[2], *self.action_ranges[\"snr\"])\n",
        "        return [carrier_freq, chip_rate, snr_db]\n",
        "\n",
        "    def _generate_m_sequence(self, taps, length, initial_state):\n",
        "        \"\"\"Generate an m-sequence.\"\"\"\n",
        "        lfsr = initial_state\n",
        "        seq = np.zeros(length)\n",
        "        # mask = sum(1 << (t - 1) for t in taps) # Mask is not used in this implementation\n",
        "\n",
        "        for i in range(length):\n",
        "            seq[i] = 1 if (lfsr & 1) else -1  # Convert to bipolar (-1, 1)\n",
        "            feedback = 0\n",
        "            for tap in taps:\n",
        "                feedback ^= (lfsr >> (tap - 1)) & 1\n",
        "            # Adjust the shift based on the number of bits in the initial state\n",
        "            lfsr = (lfsr >> 1) | (\n",
        "                feedback << (len(bin(initial_state)) - 3)\n",
        "            )  # Assuming initial_state is not 0\n",
        "\n",
        "        return seq\n",
        "\n",
        "    def _generate_spreading_code(self, length):\n",
        "        \"\"\"Generate a Gold sequence.\"\"\"\n",
        "        # The lengths of the m-sequences must be the same\n",
        "        # and derived from the same primitive polynomial.\n",
        "        # The taps define the primitive polynomials.\n",
        "        # The seeds are the initial states of the LFSRs.\n",
        "\n",
        "        m_seq1 = self._generate_m_sequence(self.taps1, length, self.seed1)\n",
        "        m_seq2 = self._generate_m_sequence(self.taps2, length, self.seed2)\n",
        "\n",
        "        # XOR the two m-sequences\n",
        "        gold_seq = m_seq1 * m_seq2  # For bipolar sequences, XOR is multiplication\n",
        "\n",
        "        return gold_seq\n",
        "\n",
        "    def _int_to_bits(self, decimal_value, num_bits):\n",
        "        \"\"\"Convert a decimal value to a binary string of a fixed number of bits.\"\"\"\n",
        "        binary_string = bin(decimal_value)[2:].zfill(num_bits)\n",
        "        return [int(bit) for bit in binary_string]\n",
        "\n",
        "    def _bits_to_int(self, binary_string):\n",
        "        \"\"\"Convert a binary string to a decimal value.\"\"\"\n",
        "        return int(binary_string, 2)\n",
        "\n",
        "    def _embed_bits_lsb(self, waveform, bits_to_embed, start_sample=0):\n",
        "        \"\"\"Embed a sequence of bits into the least significant bits of waveform samples.\"\"\"\n",
        "        audio_int = (waveform * NORM_NUM).astype(np.int16)\n",
        "\n",
        "        if len(bits_to_embed) + start_sample > len(audio_int):\n",
        "            raise ValueError(\"Not enough waveform samples to embed all bits.\")\n",
        "\n",
        "        # Create a copy to modify\n",
        "        audio_int_modified = audio_int.copy()\n",
        "\n",
        "        # Embed the bits starting from start_sample\n",
        "        for i, bit in enumerate(bits_to_embed):\n",
        "            if start_sample + i < len(audio_int_modified):\n",
        "                # Replace the LSB of each 16-bit sample\n",
        "                sample_index = start_sample + i\n",
        "                # Clear the LSB and set it to the new bit\n",
        "                audio_int_modified[sample_index] = (\n",
        "                    audio_int_modified[sample_index] & 0xFE\n",
        "                ) | bit\n",
        "\n",
        "        return audio_int_modified.astype(np.float32) / NORM_NUM\n",
        "\n",
        "    def _extract_bits_lsb(self, waveform, num_bits_to_extract, start_sample=0):\n",
        "        \"\"\"Extract a sequence of bits from the least significant bits of waveform samples.\"\"\"\n",
        "        audio_int = (waveform * NORM_NUM).astype(np.int16)\n",
        "\n",
        "        if num_bits_to_extract + start_sample > len(audio_int):\n",
        "            raise ValueError(\"Cannot extract more bits than available LSBs.\")\n",
        "\n",
        "        extracted_bits = []\n",
        "        for i in range(num_bits_to_extract):\n",
        "            sample_index = start_sample + i\n",
        "            # Extract the LSB\n",
        "            bit = audio_int[sample_index] & 1\n",
        "            extracted_bits.append(bit)\n",
        "\n",
        "        return extracted_bits\n",
        "\n",
        "    def embed(self, waveform, msg_bits: np.ndarray, action, **kwargs):\n",
        "        \"\"\"\n",
        "        Embed a message into an waveform file using spread spectrum with LSB hiding of parameters.\n",
        "\n",
        "        Parameters:\n",
        "          waveform (ndarray): Original waveform data\n",
        "          msg_bits (ndarray): Message bits to embed\n",
        "          action (tuple): Parameters for embedding (carrier_freq, chip_rate, snr_db)\n",
        "        \"\"\"\n",
        "        # Set parameters from action\n",
        "        carrier_freq, chip_rate, snr_db = action\n",
        "\n",
        "        # print(f\"Embedding parameters: carrier_freq={carrier_freq}, chip_rate={chip_rate}, snr_db={snr_db}, message_length={message_length}\")\n",
        "\n",
        "        msg_bits_bipolar = msg_bits * 2 - 1 # Convert message bits to bipolar (-1, 1)\n",
        "\n",
        "        # Calculate message length in bits\n",
        "        message_length = len(msg_bits)\n",
        "\n",
        "        # Generate spreading codes\n",
        "        code_length = message_length * chip_rate\n",
        "        spreading_code = self._generate_spreading_code(code_length)\n",
        "\n",
        "        # Create the spread message signal\n",
        "        spread_message = np.repeat(msg_bits_bipolar, chip_rate) * spreading_code\n",
        "\n",
        "        # Create carrier signal (sine wave at carrier frequency)\n",
        "        t = np.arange(len(spread_message)) / cfg.SAMPLE_RATE\n",
        "        carrier = np.sin(2 * np.pi * carrier_freq * t)\n",
        "\n",
        "        modulated = spread_message * carrier  # Modulate the message onto the carrier\n",
        "\n",
        "        # Adjust the signal power based on desired SNR\n",
        "        signal_power = np.var(waveform)\n",
        "        message_power = np.var(modulated)\n",
        "        # Add a small epsilon to avoid division by zero if message_power is 0\n",
        "        epsilon = 1e-8\n",
        "        desired_message_power = signal_power / (10 ** (snr_db / 10))\n",
        "        scaling_factor = np.sqrt(desired_message_power / (message_power + epsilon))\n",
        "        modulated = modulated * scaling_factor\n",
        "\n",
        "        stego_waveform = waveform.copy()\n",
        "\n",
        "        # Calculate the start sample for parameter embedding at the end\n",
        "        param_start_sample = len(stego_waveform) - cfg.TOTAL_PARAM_SIZE\n",
        "\n",
        "        # Pad or truncate the modulated signal to match waveform length,\n",
        "        # excluding the last TOTAL_PARAM_SIZE samples\n",
        "        audio_segment_length = len(stego_waveform) - cfg.TOTAL_PARAM_SIZE\n",
        "        if len(modulated) < audio_segment_length:\n",
        "            modulated = np.pad(\n",
        "                modulated, (0, audio_segment_length - len(modulated)), \"constant\"\n",
        "            )\n",
        "        else:\n",
        "            modulated = modulated[:audio_segment_length]\n",
        "\n",
        "        # Add the modulated signal to the waveform (excluding the last section)\n",
        "        stego_waveform[:audio_segment_length] = stego_waveform[:audio_segment_length] + modulated\n",
        "\n",
        "        # Convert parameters to binary strings for LSB embedding\n",
        "        carrier_freq_bits = self._int_to_bits(carrier_freq, cfg.CARRIER_FREQ_SIZE)\n",
        "        chip_rate_bits = self._int_to_bits(chip_rate, cfg.CHIP_RATE_SIZE)\n",
        "        snr_db_bits = self._int_to_bits(snr_db, cfg.SNR_DB_SIZE)\n",
        "        message_length_bits = self._int_to_bits(message_length, cfg.MSG_SIZE)\n",
        "\n",
        "        # Concatenate all parameter bits\n",
        "        param_bits = (\n",
        "            carrier_freq_bits + chip_rate_bits + snr_db_bits + message_length_bits\n",
        "        )\n",
        "\n",
        "        # Embed parameter bits in the last samples\n",
        "        stego_waveform = self._embed_bits_lsb(\n",
        "            stego_waveform, param_bits, start_sample=param_start_sample\n",
        "        )\n",
        "\n",
        "        return stego_waveform\n",
        "\n",
        "    def extract_param_bits(self, bits, start=None, end=None) -> int:\n",
        "        \"\"\"Extract parameter bits from a list of bits.\"\"\"\n",
        "        if start is None:\n",
        "            extracted = bits[:end]\n",
        "        if end is None:\n",
        "            extracted = bits[start:]\n",
        "        extracted = bits[start:end]\n",
        "        to_str = \"\".join(str(bit) for bit in extracted)\n",
        "        return self._bits_to_int(to_str)\n",
        "\n",
        "    def extract(self, stego_waveform, original_audio_path=None, **kwargs):\n",
        "        \"\"\"\n",
        "        Extract a hidden message from a stego waveform file.\n",
        "\n",
        "        Parameters:\n",
        "            stego_waveform (ndarray): Stego waveform data\n",
        "            message_length (int): Optional length of the hidden message in bits\n",
        "            original_audio_path (str): Optional path to original waveform for comparison\n",
        "        \"\"\"\n",
        "        # Extract parameter bits from the last samples\n",
        "        param_start_sample = len(stego_waveform) - cfg.TOTAL_PARAM_SIZE\n",
        "        extracted_param_bits = self._extract_bits_lsb(\n",
        "            stego_waveform, cfg.TOTAL_PARAM_SIZE, start_sample=param_start_sample\n",
        "        )\n",
        "\n",
        "        start, end = 0, cfg.CARRIER_FREQ_SIZE\n",
        "        carrier_freq = self.extract_param_bits(extracted_param_bits, start, end)\n",
        "        start, end = cfg.CARRIER_FREQ_SIZE, cfg.CARRIER_FREQ_SIZE + cfg.CHIP_RATE_SIZE\n",
        "        chip_rate = self.extract_param_bits(extracted_param_bits, start, end)\n",
        "        start, end = (\n",
        "            cfg.CARRIER_FREQ_SIZE + cfg.CHIP_RATE_SIZE,\n",
        "            cfg.CARRIER_FREQ_SIZE + cfg.CHIP_RATE_SIZE + cfg.SNR_DB_SIZE,\n",
        "        )\n",
        "        snr_db = self.extract_param_bits(extracted_param_bits, start, end)\n",
        "        start, end = cfg.CARRIER_FREQ_SIZE + cfg.CHIP_RATE_SIZE + cfg.SNR_DB_SIZE, None\n",
        "        message_length = self.extract_param_bits(extracted_param_bits, start, end)\n",
        "\n",
        "        # print(f\"Extracted parameters: carrier_freq={carrier_freq}, chip_rate={chip_rate}, snr_db={snr_db}, message_length={message_length}\")\n",
        "\n",
        "        # If original waveform is provided, subtract it to get just the message\n",
        "        # if original_audio_path:\n",
        "        #     y_original, _ = librosa.load(original_audio_path, sr=cfg.SAMPLE_RATE)\n",
        "        #     # Ensure lengths match before subtraction\n",
        "        #     min_len = min(len(stego_waveform), len(y_original))\n",
        "        #     y_diff = stego_waveform[:min_len] - y_original[:min_len]\n",
        "        # else:\n",
        "        y_diff = stego_waveform\n",
        "\n",
        "        # Generate the same spreading code used in embedding\n",
        "        code_length = message_length * chip_rate\n",
        "        spreading_code = self._generate_spreading_code(code_length)\n",
        "\n",
        "        # Create carrier signal\n",
        "        t = np.arange(len(spreading_code)) / cfg.SAMPLE_RATE\n",
        "        carrier = np.sin(2 * np.pi * carrier_freq * t)\n",
        "\n",
        "        # Pad or truncate the carrier to match the difference signal length,\n",
        "        # excluding the last TOTAL_PARAM_SIZE samples\n",
        "        audio_segment_length = len(y_diff) - cfg.TOTAL_PARAM_SIZE\n",
        "        if len(carrier) < audio_segment_length:\n",
        "            carrier = np.pad(carrier, (0, audio_segment_length - len(carrier)), \"constant\")\n",
        "        else:\n",
        "            carrier = carrier[:audio_segment_length]\n",
        "\n",
        "\n",
        "        # Demodulate the signal from the relevant waveform segment\n",
        "        demodulated = y_diff[:audio_segment_length] * carrier\n",
        "\n",
        "        # Correlate with spreading code to extract bits\n",
        "        extracted_bits = []\n",
        "        for i in range(message_length):\n",
        "            start = i * chip_rate\n",
        "            end = start + chip_rate\n",
        "            if end > len(demodulated):\n",
        "                break\n",
        "            segment = demodulated[start:end]\n",
        "            code_segment = spreading_code[start:end]\n",
        "\n",
        "            # Calculate correlation\n",
        "            correlation = np.sum(segment * code_segment)\n",
        "            extracted_bits.append(1 if correlation > 0 else 0)\n",
        "\n",
        "        return extracted_bits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfre1MB1Julg"
      },
      "source": [
        "# --- MODULE 3: RL ACTOR-CRITIC NETWORKS ---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmJ4bvTvJeK2"
      },
      "outputs": [],
      "source": [
        "class TransferPolicyNetwork(nn.Module):\n",
        "  def __init__(self, pretrained_feature_extractor):\n",
        "    super().__init__()\n",
        "    self.feature_extractor = pretrained_feature_extractor\n",
        "    for param in self.feature_extractor.parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "    self.policy_head = nn.Sequential(\n",
        "        nn.Linear(self.feature_extractor.combiner.out_features, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 1)\n",
        "    )\n",
        "\n",
        "  def forward(self, observations):\n",
        "    features = self.feature_extractor(observations)\n",
        "    return self.policy_head(features)\n",
        "\n",
        "# Example usage with VGGish (audio feature extractor)\n",
        "# vggish = torch.hub.load('harritaylor/torchvggish', 'vggish')\n",
        "# policy_net = TransferPolicyNetwork(vggish).to(device)\n",
        "\n",
        "# 7. Feature Extractor for PPO\n",
        "class AudioFeatureExtractor(BaseFeaturesExtractor):\n",
        "    def __init__(self, observation_space: spaces.Dict):\n",
        "        super().__init__(observation_space, features_dim=256)\n",
        "\n",
        "        # Feature-specific processing\n",
        "        self.mfcc_net = nn.Sequential(\n",
        "            nn.Linear(observation_space['mfcc'].shape[0], 128),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.sc_net = nn.Linear(1, 32)\n",
        "        self.rms_net = nn.Linear(1, 32)\n",
        "        self.zcr_net = nn.Linear(1, 32)\n",
        "        self.msg_net = nn.Linear(1, 32)\n",
        "\n",
        "        # Combined processing\n",
        "        self.combined = nn.Sequential(\n",
        "            nn.Linear(128+32*4, 256),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, observations: dict[str, torch.Tensor]) -> torch.Tensor:\n",
        "        mfcc = self.mfcc_net(observations[\"mfcc\"])\n",
        "        sc = self.sc_net(observations[\"spectral_centroid\"])\n",
        "        rms = self.rms_net(observations[\"rms\"])\n",
        "        zcr = self.zcr_net(observations[\"zcr\"])\n",
        "        msg = self.msg_net(observations[\"bits_len\"])\n",
        "\n",
        "        combined = torch.cat([mfcc, sc, rms, zcr, msg], dim=1)\n",
        "        return self.combined(combined)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EH5KOS4wJw0g"
      },
      "source": [
        "# --- MODULE 4: ENVIRONMENT NETWORK (STEGANALYZER) ---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transfer Steganalysis"
      ],
      "metadata": {
        "id": "tSLvRZNIzkAS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate a random message (for the RL environment)\n",
        "def generate_random_message(length=50):\n",
        "    \"\"\"Generates a random string message.\"\"\"\n",
        "    import string\n",
        "    letters = string.ascii_letters + string.digits + string.punctuation + \" \"\n",
        "    return ''.join(random.choice(letters) for i in range(length))\n",
        "\n",
        "def extract_audio_features(waveform, bits_len):\n",
        "    features = {}\n",
        "    features['mfcc'] = librosa.feature.mfcc(\n",
        "        y=waveform, sr=cfg.SAMPLE_RATE,\n",
        "        n_mfcc=cfg.N_MELS,\n",
        "        n_fft=cfg.FRAME_SIZE, hop_length=cfg.HOP_LENGTH\n",
        "    ).mean(axis=1)\n",
        "\n",
        "    features['spectral_centroid'] = librosa.feature.spectral_centroid(\n",
        "        y=waveform, sr=cfg.SAMPLE_RATE\n",
        "    ).mean(keepdims=True)\n",
        "\n",
        "    features['rms'] = librosa.feature.rms(y=waveform).mean(keepdims=True)\n",
        "    features['zcr'] = librosa.feature.zero_crossing_rate(y=waveform).mean(keepdims=True)\n",
        "    features[\"bits_len\"] = bits_len/1000\n",
        "\n",
        "    return features"
      ],
      "metadata": {
        "id": "fZuDBaKGXaM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransferSteganalysis(nn.Module):\n",
        "  def __init__(self, pretrained_cnn):\n",
        "    super().__init__()\n",
        "    self.features = pretrained_cnn.features[:8] # first 8 layers\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.AdaptiveAvgPool1d(output_size=1),\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(512, 1),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.features(x)\n",
        "    return self.classifier(x)\n",
        "\n",
        "# Example with pre-trained audio CNN\n",
        "# pretrained = torch.hub.load('pytorch/vision', 'resnet18', pretrained=True)\n",
        "# steganalysis_net = TransferSteganalysis(pretrained).to(device)"
      ],
      "metadata": {
        "id": "OwRDsnqdznwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom Environment"
      ],
      "metadata": {
        "id": "TSOQmgvV0XoD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vk-lzxwRJt0P"
      },
      "outputs": [],
      "source": [
        " #5. Custom Gym Environment for RL Training\n",
        "class AudioStegoEnv(gym.Env):\n",
        "  def __init__(self, dataset, method=\"sign-encoding\"):\n",
        "    super(AudioStegoEnv, self).__init__()\n",
        "    self.dataset = dataset\n",
        "\n",
        "    self.action_space = spaces.Box(low=0.0, high=0.1, shape=(1,), dtype=np.float32)\n",
        "    if method == \"spread-spectrum\":\n",
        "        self.action_space = spaces.Box(low=0, high=1, shape=(3,), dtype=np.float32)\n",
        "\n",
        "    # Define observation space as a dictionary for MultiInputPolicy\n",
        "    mfcc_dim = cfg.N_MELS\n",
        "    spectral_centroid_dim = 1 # mean over frames\n",
        "    rms_dim = 1 # mean over frames\n",
        "    zcr_dim = 1 # mean over frames\n",
        "    msg_dim = 1\n",
        "\n",
        "    self.observation_space = spaces.Dict({\n",
        "            \"mfcc\": spaces.Box(low=-np.inf, high=np.inf, shape=(cfg.N_MELS,)),\n",
        "            \"spectral_centroid\": spaces.Box(low=0, high=np.inf, shape=(1,)),\n",
        "            \"rms\": spaces.Box(low=0, high=np.inf, shape=(1,)),\n",
        "            \"zcr\": spaces.Box(low=0, high=np.inf, shape=(1,)),\n",
        "            \"bits_len\": spaces.Box(low=0, high=np.inf, shape=(1,)),\n",
        "        })\n",
        "\n",
        "    self.embedder: EmbeddingModule = SignEncoding() if method == \"sign-encoding\" else SpreadSpectrum()\n",
        "\n",
        "    # Initialize state\n",
        "    # self.reset()\n",
        "\n",
        "  def reset_with_audio(self, audio_path, msg):\n",
        "    self.audio_path = audio_path\n",
        "    self.original_waveform = AudioPreprocessor.load_audio(audio_path)\n",
        "    self.msg_bits = string_to_bits(msg)\n",
        "    self.bits_len = len(self.msg_bits)\n",
        "    self.current_waveform = self.original_waveform.copy()\n",
        "    self.current_step = 0\n",
        "    return self._get_obs(), {} # Return observation and info dictionary\n",
        "\n",
        "  def reset(self, seed=None, options=None):\n",
        "    \"\"\"Reset environment to initial state\"\"\"\n",
        "    super().reset(seed=seed)\n",
        "    msg_len = int(random.random()*100 + 50)\n",
        "    self.msg = generate_random_message(msg_len)\n",
        "    self.msg_bits = string_to_bits(self.msg)\n",
        "    self.bits_len = len(self.msg_bits)\n",
        "    tensor, sr, _ = random.choice(self.dataset)\n",
        "    self.original_waveform = AudioPreprocessor.resample_audio(tensor[0].numpy(), sr=sr)\n",
        "\n",
        "    self.current_waveform = self.original_waveform.copy()\n",
        "    # self.current_step = 0\n",
        "    return self._get_obs(), {} # Return observation and info dictionary\n",
        "\n",
        "  def _get_obs(self):\n",
        "    \"\"\"Extract features and concatenate them as observation\"\"\"\n",
        "    return extract_audio_features(self.current_waveform, self.bits_len)\n",
        "\n",
        "\n",
        "\n",
        "  def step(self, action):\n",
        "    \"\"\"Execute one embedding step\"\"\"\n",
        "    action = self.embedder.set_parameters(action)\n",
        "\n",
        "    # Embed message\n",
        "    self.current_waveform = self.embedder.embed(msg_bits = self.msg_bits, waveform=self.original_waveform, action=action)\n",
        "\n",
        "    # Compute rewards\n",
        "    snr = self._calculate_snr()\n",
        "    # psnr = self._calculate_psnr()\n",
        "    # detection_prob = self._simulate_detection()\n",
        "    accuracy, ber = self._calculate_accuracy_ber()\n",
        "    reward = self._calculate_reward(snr,ber, 0, accuracy)\n",
        "\n",
        "    # Update state\n",
        "    # self.current_step += 1\n",
        "    # done = self.current_step >= 10\n",
        "    info = {\n",
        "        \"snr\": snr,\n",
        "        # \"psnr\": psnr,\n",
        "        \"ber\": ber,\n",
        "        # \"detection_prob\": detection_prob,\n",
        "        \"extraction_accuracy\": accuracy,\n",
        "        \"reward\": reward,\n",
        "        \"action\": action\n",
        "    }\n",
        "    return self._get_obs(), reward, True, None, info\n",
        "\n",
        "  def _calculate_snr(self):\n",
        "    \"\"\"Calculate Signal-to-Noise Ratio\"\"\"\n",
        "    # Ensure both audio arrays have the same length before calculating noise\n",
        "    min_len = min(len(self.current_waveform), len(self.original_waveform))\n",
        "    current_audio_trimmed = self.current_waveform[:min_len]\n",
        "    original_audio_trimmed = self.original_waveform[:min_len]\n",
        "\n",
        "    noise = current_audio_trimmed - original_audio_trimmed\n",
        "    signal_power = np.mean(original_audio_trimmed ** 2)\n",
        "    noise_power = np.mean(noise ** 2)\n",
        "\n",
        "    if noise_power == 0:\n",
        "        return 100  # High SNR if no noise\n",
        "\n",
        "    return 10 * np.log10(signal_power / noise_power)\n",
        "\n",
        "  # def _calculate_psnr(self):\n",
        "  #   if len(self.current_waveform) < len(self.original_waveform):\n",
        "  #       stego_waveform_padded = np.pad(self.current_waveform, (0, len(self.original_waveform) - len(self.current_waveform)), 'constant')\n",
        "  #   else:\n",
        "  #       stego_waveform_padded = self.current_waveform[:len(self.original_waveform)]\n",
        "\n",
        "  #   psnr = 10 * np.log10(np.max(self.original_waveform ** 2) / np.mean((self.original_waveform - stego_waveform_padded) ** 2)) if np.mean((self.original_waveform - stego_waveform_padded) ** 2) > 0 else float('inf')\n",
        "  #   return psnr\n",
        "\n",
        "\n",
        "  def _simulate_detection(self):\n",
        "    pass\n",
        "  #   \"\"\"Simulate steganalysis detection\"\"\"\n",
        "  #   # not real\n",
        "  #   snr = self._calculate_snr()\n",
        "  #   return 1 / (1 + np.exp(0.5 * (snr - 50)))  # Logistic function\n",
        "\n",
        "  def _calculate_accuracy_ber(self):\n",
        "    \"\"\"Calculate the accuracy of extracted bits\"\"\"\n",
        "    extracted_bits = self.embedder.extract(stego_waveform =self.current_waveform, bits_len=self.bits_len)\n",
        "    bits_len = len(extracted_bits)\n",
        "\n",
        "    min_len_bits = min(bits_len, self.bits_len)\n",
        "    ber = np.mean([self.msg_bits[i] != extracted_bits[i] for i in range(min_len_bits)]) if min_len_bits > 0 else 1.0 # BER is 1 if no bits to compare\n",
        "    extracted_str = bits_to_string(extracted_bits)\n",
        "    # print(f\"Original message: {self.msg}\")\n",
        "    # print(f\"Extracted message: {extracted_str}\")\n",
        "    str_len = len(extracted_str)\n",
        "    bits_len = len(self.msg)\n",
        "    min_len_str = min(str_len, bits_len)\n",
        "    str_ber = np.mean([self.msg[i] != extracted_str[i] for i in range(min_len_str)]) if min_len_str > 0 else 1.0 # BER is 1 if no bits to compare\n",
        "\n",
        "    return 1.0 - str_ber, ber\n",
        "\n",
        "  def _calculate_reward(self, snr,ber, detection_prob, extraction_accuracy):\n",
        "    \"\"\"Calculate reward balancing SNR, detectability, and extraction accuracy\"\"\"\n",
        "    # Target: SNR > 50 dB, detection_prob < 0.1, extraction_accuracy close to 1.0\n",
        "    snr_reward = min(snr / 50, 1.0)\n",
        "    # detect_reward = 1.0 - min(detection_prob, 1.0)\n",
        "\n",
        "    # Weighted combination\n",
        "    return (\n",
        "        cfg.SNR_WEIGHTS * snr_reward - ber +\n",
        "        # cfg.DETECTION_WEIGHT * detect_reward +\n",
        "        cfg.EXTRACTION_ACCURACY_WEIGHT * extraction_accuracy\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# -- Prepare sample audios --"
      ],
      "metadata": {
        "id": "RKny7R_ZoaM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import IPython\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sample_dir = \"_assets\"\n",
        "os.makedirs(sample_dir, exist_ok=True)\n",
        "\n",
        "def plot_specgram(waveform, sample_rate, title=\"Spectrogram\", xlim=None):\n",
        "  waveform = waveform.numpy()\n",
        "  figure, ax = plt.subplots()\n",
        "  ax.specgram(waveform[0], Fs=sample_rate)\n",
        "  figure.suptitle(title)\n",
        "  figure.tight_layout()\n",
        "\n",
        "dataset = torchaudio.datasets.YESNO(sample_dir, download=True)\n",
        "sample_path = \"_assets/waves_yesno/0_0_0_0_1_1_1_1.wav\""
      ],
      "metadata": {
        "id": "PdIDqIVfWReT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10a81e99-6365-4bd8-ad48-e2a1838c1e90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.49M/4.49M [00:01<00:00, 3.73MB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/torchaudio/datasets/utils.py:25: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "  tar.extract(file_, to_path)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# i = 8\n",
        "# waveform, sr, label = dataset[i]\n",
        "# processor = AudioPreprocessor()\n",
        "# waveform = AudioPreprocessor.resample_audio(waveform[0].numpy(), sr)\n",
        "# features = extract_audio_features(waveform, 500)\n",
        "# print(waveform.shape)\n",
        "# mod = waveform.copy()[:-98]\n",
        "# mod.shape"
      ],
      "metadata": {
        "id": "yqbrpSgJBole"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# env = AudioStegoEnv(dataset, method=\"spread-spectrum\")\n",
        "# # env = AudioStegoEnv(dataset, method=\"sign-encoding\")\n",
        "# obs, info = env.reset()\n",
        "# obs, reward, done, truncated, info = env.step([0.5, 0.2, 0])\n",
        "# info"
      ],
      "metadata": {
        "id": "-5K4hSX51J2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A69oR2apRBwA"
      },
      "source": [
        "# -- MAIN FRAMEWORK --"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWcvIP6yRIBc"
      },
      "outputs": [],
      "source": [
        "class RLAudioSteganography:\n",
        "  \"\"\"Main framework class\"\"\"\n",
        "  def __init__(self, cfg: Config, method=\"sign-encoding\") -> None:\n",
        "    self.cfg = cfg\n",
        "    self.method = method\n",
        "    self.embedder: EmbeddingModule = SignEncoding() if method == \"sign-encoding\" else SpreadSpectrum()\n",
        "\n",
        "\n",
        "  def Initialize_components(self, audio_path, method=\"sign-encoding\"):\n",
        "    \"\"\"Initialize components\"\"\"\n",
        "    # self.steganalysis_net = SteganalysisCNN()\n",
        "    # self.steganalysis_net = SteganalysisCNN(input_channels=len(self.preprocessor.audio))\n",
        "    # self.steganalysis_net.to(device)\n",
        "\n",
        "  def train_ppo(self, total_timesteps=10000)-> PPO:\n",
        "  # def train_ppo(self, total_timesteps=25000)-> PPO:\n",
        "    \"\"\"Train PPO agent for audio steganography\"\"\"\n",
        "    env = make_vec_env(lambda: AudioStegoEnv(dataset, method=self.method), n_envs=4)\n",
        "\n",
        "    policy_kwargs = dict(\n",
        "        features_extractor_class=AudioFeatureExtractor,\n",
        "        features_extractor_kwargs=dict(),\n",
        "        net_arch=dict(pi=[256, 256], vf=[256, 256])\n",
        "    )\n",
        "    model = PPO(\n",
        "        \"MultiInputPolicy\",\n",
        "        env,\n",
        "        policy_kwargs=policy_kwargs,\n",
        "        verbose=1,\n",
        "        device=device,\n",
        "        learning_rate=cfg.LEARNING_RATE,\n",
        "      )\n",
        "    # Instantiate the custom callback\n",
        "    custom_callback = CustomLoggingCallback()\n",
        "    model.learn(cfg.EPISODES, custom_callback)\n",
        "\n",
        "    return model\n",
        "\n",
        "  def embed_message(self, audio_path, message, output_path, model):\n",
        "    \"\"\"Embed a message into an audio file using trained policy\"\"\"\n",
        "    # Re-initialize preprocessor and compute magnitudes/phases/mask for the specific audio being embedded into\n",
        "    waveform = AudioPreprocessor.load_audio(audio_path)\n",
        "\n",
        "    msg_bits = string_to_bits(message)\n",
        "    env = AudioStegoEnv([],self.method) # Use the correct audio_path and message\n",
        "    obs, info = env.reset_with_audio(audio_path, message)\n",
        "    action, _ = model.predict(obs, deterministic=True)\n",
        "    action = self.embedder.set_parameters(action[0])\n",
        "    # Convert message string to bits for embedding\n",
        "    message_bits = string_to_bits(message)\n",
        "    stego_waveform = self.embedder.embed( msg_bits = msg_bits, action = action, waveform=waveform)\n",
        "    AudioPreprocessor.save_audio(stego_waveform,cfg.SAMPLE_RATE, output_path)\n",
        "    print(f\"Stego audio saved to {output_path}\")\n",
        "\n",
        "\n",
        "  def extract_message(self, stego_audio_path, msg_length):\n",
        "    \"\"\"Extract a message from a stego audio file\"\"\"\n",
        "    # Load the stego audio\n",
        "    waveform = AudioPreprocessor.load_audio(stego_audio_path)\n",
        "\n",
        "    extracted_bits = self.embedder.extract(stego_waveform=waveform, message_length= msg_length)\n",
        "    return bits_to_string(extracted_bits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULjz6C-CKiMq"
      },
      "source": [
        "# --- MAIN EXECUTION SCRIPT ---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "srWhvcBg59Qv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ac03a9e-ef30-4d0c-e104-8cec60983920"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training PPO agent...\n",
            "Using cuda device\n",
            "-----------------------------------------------\n",
            "| rollout/                  |                 |\n",
            "|    ep_action              | [11196, 44, 20] |\n",
            "|    ep_ber                 | 0.0667          |\n",
            "|    ep_extraction_accuracy | 0.819           |\n",
            "|    ep_len_mean            | 1               |\n",
            "|    ep_rew_mean            | 0.304           |\n",
            "|    ep_reward              | 0.603           |\n",
            "|    ep_snr                 | 22.21757        |\n",
            "| time/                     |                 |\n",
            "|    fps                    | 4               |\n",
            "|    iterations             | 1               |\n",
            "|    time_elapsed           | 1862            |\n",
            "|    total_timesteps        | 8192            |\n",
            "-----------------------------------------------\n",
            "----------------------------------------------\n",
            "| rollout/                  |                |\n",
            "|    ep_action              | [15000, 3, 20] |\n",
            "|    ep_ber                 | 0.0108         |\n",
            "|    ep_extraction_accuracy | 0.948          |\n",
            "|    ep_len_mean            | 1              |\n",
            "|    ep_rew_mean            | 0.454          |\n",
            "|    ep_reward              | 0.777          |\n",
            "|    ep_snr                 | 27.363544      |\n",
            "| time/                     |                |\n",
            "|    fps                    | 4              |\n",
            "|    iterations             | 2              |\n",
            "|    time_elapsed           | 3693           |\n",
            "|    total_timesteps        | 16384          |\n",
            "| train/                    |                |\n",
            "|    approx_kl              | 0.21269172     |\n",
            "|    clip_fraction          | 0.715          |\n",
            "|    clip_range             | 0.2            |\n",
            "|    entropy_loss           | -4.12          |\n",
            "|    explained_variance     | -0.299         |\n",
            "|    learning_rate          | 0.0003         |\n",
            "|    loss                   | -0.0495        |\n",
            "|    n_updates              | 10             |\n",
            "|    policy_gradient_loss   | -0.137         |\n",
            "|    std                    | 0.936          |\n",
            "|    value_loss             | 0.198          |\n",
            "----------------------------------------------\n",
            "---------------------------------------------\n",
            "| rollout/                  |               |\n",
            "|    ep_action              | [5000, 3, 26] |\n",
            "|    ep_ber                 | 0.0389        |\n",
            "|    ep_extraction_accuracy | 0.797         |\n",
            "|    ep_len_mean            | 1             |\n",
            "|    ep_rew_mean            | 0.525         |\n",
            "|    ep_reward              | 0.669         |\n",
            "|    ep_snr                 | 28.638977     |\n",
            "| time/                     |               |\n",
            "|    fps                    | 4             |\n",
            "|    iterations             | 3             |\n",
            "|    time_elapsed           | 5437          |\n",
            "|    total_timesteps        | 24576         |\n",
            "| train/                    |               |\n",
            "|    approx_kl              | 0.08567856    |\n",
            "|    clip_fraction          | 0.569         |\n",
            "|    clip_range             | 0.2           |\n",
            "|    entropy_loss           | -3.88         |\n",
            "|    explained_variance     | -1.19e-07     |\n",
            "|    learning_rate          | 0.0003        |\n",
            "|    loss                   | -0.0508       |\n",
            "|    n_updates              | 20            |\n",
            "|    policy_gradient_loss   | -0.124        |\n",
            "|    std                    | 0.882         |\n",
            "|    value_loss             | 0.145         |\n",
            "---------------------------------------------\n",
            "---------------------------------------------\n",
            "| rollout/                  |               |\n",
            "|    ep_action              | [5000, 3, 20] |\n",
            "|    ep_ber                 | 0.00379       |\n",
            "|    ep_extraction_accuracy | 0.97          |\n",
            "|    ep_len_mean            | 1             |\n",
            "|    ep_rew_mean            | 0.655         |\n",
            "|    ep_reward              | 0.808         |\n",
            "|    ep_snr                 | 28.716774     |\n",
            "| time/                     |               |\n",
            "|    fps                    | 4             |\n",
            "|    iterations             | 4             |\n",
            "|    time_elapsed           | 6843          |\n",
            "|    total_timesteps        | 32768         |\n",
            "| train/                    |               |\n",
            "|    approx_kl              | 0.087434396   |\n",
            "|    clip_fraction          | 0.591         |\n",
            "|    clip_range             | 0.2           |\n",
            "|    entropy_loss           | -3.64         |\n",
            "|    explained_variance     | -1.19e-07     |\n",
            "|    learning_rate          | 0.0003        |\n",
            "|    loss                   | -0.0504       |\n",
            "|    n_updates              | 30            |\n",
            "|    policy_gradient_loss   | -0.101        |\n",
            "|    std                    | 0.822         |\n",
            "|    value_loss             | 0.102         |\n",
            "---------------------------------------------\n",
            "----------------------------------------------\n",
            "| rollout/                  |                |\n",
            "|    ep_action              | [15000, 3, 20] |\n",
            "|    ep_ber                 | 0.00188        |\n",
            "|    ep_extraction_accuracy | 0.985          |\n",
            "|    ep_len_mean            | 1              |\n",
            "|    ep_rew_mean            | 0.694          |\n",
            "|    ep_reward              | 0.819          |\n",
            "|    ep_snr                 | 28.678917      |\n",
            "| time/                     |                |\n",
            "|    fps                    | 5              |\n",
            "|    iterations             | 5              |\n",
            "|    time_elapsed           | 7998           |\n",
            "|    total_timesteps        | 40960          |\n",
            "| train/                    |                |\n",
            "|    approx_kl              | 0.06680666     |\n",
            "|    clip_fraction          | 0.542          |\n",
            "|    clip_range             | 0.2            |\n",
            "|    entropy_loss           | -3.41          |\n",
            "|    explained_variance     | 0              |\n",
            "|    learning_rate          | 0.0003         |\n",
            "|    loss                   | -0.0758        |\n",
            "|    n_updates              | 40             |\n",
            "|    policy_gradient_loss   | -0.0909        |\n",
            "|    std                    | 0.768          |\n",
            "|    value_loss             | 0.0757         |\n",
            "----------------------------------------------\n",
            "---------------------------------------------\n",
            "| rollout/                  |               |\n",
            "|    ep_action              | [5000, 3, 20] |\n",
            "|    ep_ber                 | 0.0119        |\n",
            "|    ep_extraction_accuracy | 0.912         |\n",
            "|    ep_len_mean            | 1             |\n",
            "|    ep_rew_mean            | 0.724         |\n",
            "|    ep_reward              | 0.824         |\n",
            "|    ep_snr                 | 36.040638     |\n",
            "| time/                     |               |\n",
            "|    fps                    | 5             |\n",
            "|    iterations             | 6             |\n",
            "|    time_elapsed           | 9005          |\n",
            "|    total_timesteps        | 49152         |\n",
            "| train/                    |               |\n",
            "|    approx_kl              | 0.037073195   |\n",
            "|    clip_fraction          | 0.405         |\n",
            "|    clip_range             | 0.2           |\n",
            "|    entropy_loss           | -3.23         |\n",
            "|    explained_variance     | 5.96e-08      |\n",
            "|    learning_rate          | 0.0003        |\n",
            "|    loss                   | -0.0215       |\n",
            "|    n_updates              | 50            |\n",
            "|    policy_gradient_loss   | -0.0691       |\n",
            "|    std                    | 0.734         |\n",
            "|    value_loss             | 0.0568        |\n",
            "---------------------------------------------\n",
            "-----------------------------------------------\n",
            "| rollout/                  |                 |\n",
            "|    ep_action              | [11390, 89, 20] |\n",
            "|    ep_ber                 | 0.0976          |\n",
            "|    ep_extraction_accuracy | 0.684           |\n",
            "|    ep_len_mean            | 1               |\n",
            "|    ep_rew_mean            | 0.737           |\n",
            "|    ep_reward              | 0.491           |\n",
            "|    ep_snr                 | 22.201674       |\n",
            "| time/                     |                 |\n",
            "|    fps                    | 5               |\n",
            "|    iterations             | 7               |\n",
            "|    time_elapsed           | 9938            |\n",
            "|    total_timesteps        | 57344           |\n",
            "| train/                    |                 |\n",
            "|    approx_kl              | 0.027262766     |\n",
            "|    clip_fraction          | 0.284           |\n",
            "|    clip_range             | 0.2             |\n",
            "|    entropy_loss           | -3.08           |\n",
            "|    explained_variance     | 0               |\n",
            "|    learning_rate          | 0.0003          |\n",
            "|    loss                   | -0.0125         |\n",
            "|    n_updates              | 60              |\n",
            "|    policy_gradient_loss   | -0.0488         |\n",
            "|    std                    | 0.704           |\n",
            "|    value_loss             | 0.0459          |\n",
            "-----------------------------------------------\n",
            "---------------------------------------------\n",
            "| rollout/                  |               |\n",
            "|    ep_action              | [5000, 3, 20] |\n",
            "|    ep_ber                 | 0.00102       |\n",
            "|    ep_extraction_accuracy | 0.992         |\n",
            "|    ep_len_mean            | 1             |\n",
            "|    ep_rew_mean            | 0.754         |\n",
            "|    ep_reward              | 0.813         |\n",
            "|    ep_snr                 | 27.36309      |\n",
            "| time/                     |               |\n",
            "|    fps                    | 6             |\n",
            "|    iterations             | 8             |\n",
            "|    time_elapsed           | 10832         |\n",
            "|    total_timesteps        | 65536         |\n",
            "| train/                    |               |\n",
            "|    approx_kl              | 0.01949718    |\n",
            "|    clip_fraction          | 0.218         |\n",
            "|    clip_range             | 0.2           |\n",
            "|    entropy_loss           | -2.94         |\n",
            "|    explained_variance     | 0             |\n",
            "|    learning_rate          | 0.0003        |\n",
            "|    loss                   | -0.00184      |\n",
            "|    n_updates              | 70            |\n",
            "|    policy_gradient_loss   | -0.0349       |\n",
            "|    std                    | 0.681         |\n",
            "|    value_loss             | 0.0365        |\n",
            "---------------------------------------------\n",
            "Trained model saved to _assets/ppo_audio_stego_model\n"
          ]
        }
      ],
      "source": [
        "# Initialize the framework\n",
        "cfg = Config()\n",
        "framework = RLAudioSteganography(cfg, method=\"spread-spectrum\")\n",
        "\n",
        "# --- Training Phase ---\n",
        "\n",
        "# Initialize components with the training audio\n",
        "# framework.Initialize_components(sample_path, method=\"spread-spectrum\")\n",
        "\n",
        "# Train PPO agent\n",
        "print(\"Training PPO agent...\")\n",
        "model = framework.train_ppo()\n",
        "\n",
        "# Save the trained model\n",
        "model_save_path = \"_assets/ppo_audio_stego_model\"\n",
        "model.save(model_save_path)\n",
        "print(f\"Trained model saved to {model_save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# auto download model to local storage\n",
        "from google.colab import files\n",
        "files.download(f\"{model_save_path}.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "hCY6YpPuxv4j",
        "outputId": "c1bd42cc-60f3-45f5-bcae-387db38fbeda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_2d465737-24e9-4f09-b65f-e74d30a5b4d7\", \"ppo_audio_stego_model.zip\", 4229261)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eH9P_-wMHSiN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b76d69cb-c3a9-46e1-8480-4bc795a5fa6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded model from _assets/ppo_audio_stego_model\n",
            "Stego audio saved to test.wav\n",
            "\n",
            "Original Message: \n",
            "    Embed a message into an audio file.\n",
            "    Parameters:\n",
            "     audio_path (str): Path to the audio file.\n",
            "      message (str): The message to be embedded.\n",
            "      output_path (str): Path to save the embedded audio file.\n",
            "    \n",
            "Extracted Message: \n",
            "    Embed a message into an audio file.\n",
            "    Parameters:\n",
            " @  auDio_path (str): Path to the audio file.\n",
            "      message (str): The message to be embedd\rt.\n",
            "      output_path (str): Path tk 3ave the embedded audio fIle.\n",
            "    \n"
          ]
        }
      ],
      "source": [
        "cfg = Config()\n",
        "framework_test = RLAudioSteganography(cfg, method=\"spread-spectrum\")\n",
        "\n",
        "message = \"\"\"\n",
        "    Embed a message into an audio file.\n",
        "    Parameters:\n",
        "     audio_path (str): Path to the audio file.\n",
        "      message (str): The message to be embedded.\n",
        "      output_path (str): Path to save the embedded audio file.\n",
        "    \"\"\"\n",
        "output_path = \"test.wav\"\n",
        "\n",
        "# framework_test.Initialize_components(audio_path=sample_path, method=\"spread-spectrum\")\n",
        "# Load the saved model\n",
        "model_test = PPO.load(model_save_path)\n",
        "print(f\"Loaded model from {model_save_path}\")\n",
        "# Embed message using the loaded model\n",
        "framework_test.embed_message(sample_path, message, output_path, model_test)\n",
        "\n",
        "# Extract message from the new stego audio\n",
        "extracted_message = framework_test.extract_message(output_path, len(message))\n",
        "print(f\"\\nOriginal Message: {message}\")\n",
        "print(f\"Extracted Message: {extracted_message}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NjsNu_AhyLNw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}