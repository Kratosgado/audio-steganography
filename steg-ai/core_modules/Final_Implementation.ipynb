{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kratosgado/audio-steganography/blob/main/steg-ai/core_modules/Final_Implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QXp8Vn1BAu2P"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install stable-baselines3 shimmy --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "!git clone https://github.com/librosa/data.git ./audio_data\n",
        "# get audio files from librose\n",
        "audios_path = \"./audio_data/audio\"\n",
        "simple_audio_files = [os.path.join(audios_path, f) for f in os.listdir(audios_path) if f.endswith(\".ogg\")]\n",
        "complex_audio_files = [os.path.join(audios_path, f) for f in os.listdir(audios_path) if f.endswith(\".ogg\")]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3QaFlOkaE92",
        "outputId": "1dbca056-726d-4bcc-dc40-9ab3206a2287"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into './audio_data'...\n",
            "remote: Enumerating objects: 156, done.\u001b[K\n",
            "remote: Counting objects: 100% (156/156), done.\u001b[K\n",
            "remote: Compressing objects: 100% (117/117), done.\u001b[K\n",
            "remote: Total 156 (delta 61), reused 125 (delta 34), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (156/156), 14.78 MiB | 33.05 MiB/s, done.\n",
            "Resolving deltas: 100% (61/61), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.distributions import Normal\n",
        "from scipy.signal import get_window\n",
        "from scipy.fft import dct, idct\n",
        "import scipy.io.wavfile as wf\n",
        "import scipy\n",
        "import torch.nn.functional as F\n",
        "from collections import deque\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import random\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
        "from stable_baselines3.common.callbacks import BaseCallback"
      ],
      "metadata": {
        "id": "iZE9vuGCIj4l"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# warnings.filterwarnings(\"ignore\")\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "FRAME_SIZE = 1024 #2048\n",
        "HOP_LENGTH = 256 #512\n",
        "NON_CRITICAL_PERCENT = 0.1 # Modify bottom 10% of coeffs by magnitude\n",
        "STATE_DIM = 1024\n",
        "SAMPLE_RATE = 22050\n",
        "N_MELS = 128"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5uq1C49Mc3w",
        "outputId": "ec3bb2ac-cc43-4ec5-a338-a6088a1ee5de"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --- UTILITY FUNCTIONS ---"
      ],
      "metadata": {
        "id": "iNC_gfrXNLFw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NORM_NUM = 32768\n",
        "def textToBits( message):\n",
        "  return ''.join(format(ord(ch), '08b') for ch in message)\n",
        "\n",
        "\n",
        "def bitToChar(bits):\n",
        "  return chr(int(bits, 2))\n",
        "\n",
        "def textFromBits(bits):\n",
        "  text = ''\n",
        "  for bit in range(0, len(bits), 8):\n",
        "    text += bitToChar(bits[bit:bit + 8])\n",
        "  return text\n",
        "\n",
        "# Function to generate a random message (for the RL environment)\n",
        "def generate_random_message(length=50):\n",
        "    \"\"\"Generates a random string message.\"\"\"\n",
        "    import string\n",
        "    letters = string.ascii_letters + string.digits + string.punctuation + \" \"\n",
        "    return ''.join(random.choice(letters) for i in range(length))\n",
        "def extract_state(audio_segment, message_context, sr: int | float=44100):\n",
        "  # Audio features\n",
        "  mfcc = librosa.feature.mfcc(y=audio_segment, sr=sr)\n",
        "  spectral_centroid = librosa.feature.spectral_centroid(y=audio_segment, sr=sr)\n",
        "\n",
        "  # Temporal features\n",
        "  rms = librosa.feature.rms(y=audio_segment)\n",
        "  zcr = librosa.feature.zero_crossing_rate(y=audio_segment)\n",
        "\n",
        "  # Normalize and flatten features\n",
        "  return np.concatenate([mfcc.mean(axis=1),spectral_centroid.flatten(), rms.flatten(),zcr.flatten(),message_context ])\n",
        "\n",
        "class CustomLoggingCallback(BaseCallback):\n",
        "  \"\"\" A custom callback that logs additional information from the environment. \"\"\"\n",
        "  def __init__(self, verbose=0):\n",
        "    super(CustomLoggingCallback, self).__init__(verbose)\n",
        "\n",
        "  def _on_step(self) -> bool:\n",
        "    \"\"\"This method is called after each step in the environment.   \"\"\"\n",
        "    # Accessing environment infos from the VecEnv wrapper\n",
        "    if self.locals.get('infos'):\n",
        "      for info in self.locals['infos']:\n",
        "        # Ensure info is a dictionary and contains the required keys\n",
        "        if isinstance(info, dict) and 'snr' in info and 'detection_prob' in info:\n",
        "          self.logger.record('rollout/ep_snr', info['snr'])\n",
        "          self.logger.record('rollout/ep_reward', info['reward'])\n",
        "          self.logger.record('rollout/ep_snr', info['psnr'])\n",
        "          self.logger.record('rollout/ep_ber', info['ber'])\n",
        "          self.logger.record('rollout/ep_extraction_accuracy', info['extraction_accuracy'])\n",
        "          self.logger.record('rollout/ep_detection_prob', info['detection_prob'])\n",
        "          self.logger.record('rollout/ep_action', info['action'])\n",
        "    return True"
      ],
      "metadata": {
        "id": "_55FxjzNNKHD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --- HYPERPARAMETERS ---"
      ],
      "metadata": {
        "id": "HV_OdAlXNoLW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "  \"\"\"Class to hold all hyperparameters.\"\"\"\n",
        "  #Audio processing\n",
        "  FRAME_SIZE = 1024 #2048\n",
        "  HOP_LENGTH = 256 #512\n",
        "  NON_CRITICAL_PERCENT = 0.1 # Modify bottom 10% of coeffs by magnitude\n",
        "  STATE_DIM = 1024\n",
        "  SAMPLE_RATE = 22050\n",
        "  N_MELS = 128\n",
        "\n",
        "  # RL Training\n",
        "  EPISODES = 1000\n",
        "  LEARNING_RATE_ACTOR = 3e-4\n",
        "  LEARNING_RATE_CRITIC = 3e-4\n",
        "  GAMMA = 0.99 # Discount factor\n",
        "  GAE_LAMBDA = 0.95 # Lambda for Generalized Advantage Estimation\n",
        "  PPO_EPSILON = 0.2 # Epsilon for clipping in PPO\n",
        "  PPO_EPOCHS = 10 # Number of epochs for PPO update\n",
        "  BATCH_SIZE = 32\n",
        "\n",
        "  # Environment Network Pre-training\n",
        "  PRETRAIN_EPOCHS = 10\n",
        "  PRE_TRAIN_LR = 1e-3\n",
        "  PRE_TRAIN_BATCH_SIZE = 32\n",
        "\n",
        "  # Reward weights\n",
        "  SNR_WEIGHTS = 0.02 # Weight for SNR in reward. Tuned to be on a similar scale to detection prob.\n",
        "  DETECTION_WEIGHT = 1.0 # Weight for detection probability in reward.\n",
        "  IMPERCEPTIBILITY_WEIGHT = 0.3\n",
        "  UNDETECTABILITY_WEIGHT = 0.4\n",
        "  EXTRACTION_ACCURACY_WEIGHT = 0.3\n",
        "cfg = Config()"
      ],
      "metadata": {
        "id": "SLavUsivNtBI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --- MODULE 1: AUDIO PREPROCESSOR ---"
      ],
      "metadata": {
        "id": "W-feRKGXJB5t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AudioPreprocessor:\n",
        "  \"\"\"Handles audio loading, MDCT, and inverse MDCT.\"\"\"\n",
        "  def __init__(self, audio_path= None, audio_data=None, frame_size=cfg.FRAME_SIZE, hop_length=cfg.HOP_LENGTH, sr=cfg.SAMPLE_RATE):\n",
        "    if audio_path:\n",
        "      self.audio, self.sr = librosa.load(audio_path, sr=sr, mono=True)\n",
        "    elif audio_data is not None:\n",
        "      self.audio = audio_data\n",
        "      self.sr = sr\n",
        "    else:\n",
        "      raise ValueError(\"Either audio_path or audio_data must be provided\")\n",
        "\n",
        "    # Normalize audio\n",
        "    self.audio = self.audio / np.max(np.abs(self.audio))\n",
        "    self.frame_size = frame_size\n",
        "    self.hop_length = hop_length\n",
        "    self.window = get_window('hann', self.frame_size)\n",
        "    # return self.audio, self.sr\n",
        "\n",
        "  def load_audio(self, path):\n",
        "    \"\"\"Load WAV audio file\"\"\"\n",
        "    audio, _ = librosa.load(path, sr=self.sr)\n",
        "    return audio\n",
        "\n",
        "  def stft(self, audio):\n",
        "    \"\"\"Compute Short-Time Fourier Transform (STFT)\"\"\"\n",
        "    return librosa.stft(audio, n_fft=self.frame_size, hop_length=self.hop_length)\n",
        "\n",
        "  @staticmethod\n",
        "  def istft(stft_matrix):\n",
        "    \"\"\"Compute Inverse Short-Time Fourier Transform (ISTFT)\"\"\"\n",
        "    return librosa.istft(stft_matrix, hop_length=cfg.HOP_LENGTH)\n",
        "\n",
        "  def compute_mdct(self):\n",
        "    \"\"\"Compute Modified Discrete Cosine Transform (MDCT) using STFT and DCT from librosa\"\"\"\n",
        "    stft = self.stft(self.audio)\n",
        "    magnitudes = np.abs(stft)\n",
        "    phases = np.angle(stft)\n",
        "    return magnitudes, phases\n",
        "\n",
        "  def get_non_critical_coeffs(self, magnitudes, percentile=10):\n",
        "    \"\"\"Identify non-critical coefficients (lowest magnitude)\"\"\"\n",
        "    threshold = np.percentile(np.abs(magnitudes.flatten()), percentile)\n",
        "    mask = np.abs(magnitudes) < threshold\n",
        "    return mask\n",
        "\n",
        "  @staticmethod\n",
        "  def reconstruct_audio(magnitudes, phases):\n",
        "    \"\"\"Reconstruct audio from magnitude/stft matrix and phase/non_critical_coeffs\"\"\"\n",
        "    stft = magnitudes * np.exp(1j * phases)\n",
        "    reconstructed_audio = AudioPreprocessor.istft(stft)\n",
        "    return reconstructed_audio\n",
        "\n",
        "  def plot_spectrogram(self, title):\n",
        "    \"\"\"Visualize audio spectrogram\"\"\"\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    S = librosa.amplitude_to_db(np.abs(librosa.stft(self.audio)), ref=np.max)\n",
        "    librosa.display.specshow(S, sr=self.sr, x_axis='time', y_axis='log')\n",
        "    plt.colorbar(format='%+2.0f dB')\n",
        "    plt.title(title)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "  def save_audio(self, audio: np.ndarray, sr: int, path: str):\n",
        "    wf.write(path,sr, (audio * NORM_NUM).astype(np.int16))"
      ],
      "metadata": {
        "id": "z-rBWB4UJAyM"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --- MODULE 5: EMBEDDING/EXTRACTION MODULE ---\n",
        "Main Sign encoding class"
      ],
      "metadata": {
        "id": "hyIIZSInKMwW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from abc import ABC, abstractmethod\n",
        "\n",
        "class EmbeddingModule(ABC):\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  @abstractmethod\n",
        "  def embed(self, *args, **kwargs) -> np.ndarray:\n",
        "    pass\n",
        "\n",
        "  @abstractmethod\n",
        "  def extract(self, *args, **kwargs) -> list[int]:\n",
        "    pass\n",
        "\n",
        "  @abstractmethod\n",
        "  def set_parameters(self, action):\n",
        "    pass\n",
        "\n",
        "class SignEncoding(EmbeddingModule):\n",
        "  \"\"\"Embeds and extracts messages from MDCT coefficients.\"\"\"\n",
        "  def __init__(self, steganalysis_net=None):\n",
        "    \"\"\"Initialize the steganography system\"\"\"\n",
        "    self.steganalysis_net = steganalysis_net\n",
        "    if self.steganalysis_net:\n",
        "        self.steganalysis_net.to(device)\n",
        "        self.steganalysis_net.eval()\n",
        "\n",
        "  def set_parameters(self, action):\n",
        "    self.alpha = action[0]\n",
        "\n",
        "  def embed(self, magnitudes,phases, mask, msg_bits: np.ndarray, **kwargs):\n",
        "    \"\"\"Embed message using sign encoding in non-critical coefficients\"\"\"\n",
        "    # Apply mask to get non-critical coefficients\n",
        "    coeffs = magnitudes.copy()\n",
        "    non_critical = coeffs[mask]\n",
        "\n",
        "    # Ensure we have enough coefficients for the message\n",
        "    if len(non_critical) < len(msg_bits):\n",
        "        raise ValueError(\"Message too long for available non-critical coefficients\")\n",
        "\n",
        "    # Embed message using sign encoding\n",
        "    for i, bit in enumerate(msg_bits):\n",
        "        sign = 1 if bit == 1 else -1\n",
        "        non_critical[i] = sign * np.abs(non_critical[i]) * (1 + self.alpha)\n",
        "\n",
        "    # Update coefficients\n",
        "    coeffs[mask] = non_critical\n",
        "    self.magnitudes = coeffs\n",
        "\n",
        "    return AudioPreprocessor.reconstruct_audio(coeffs, phases)\n",
        "\n",
        "  def extract(self, magnitudes, mask, message_length, **kwargs):\n",
        "    \"\"\"Extract message from non-critical coefficients\"\"\"\n",
        "    # Apply mask to get non-critical coefficients\n",
        "    non_critical = self.magnitudes[mask]\n",
        "\n",
        "    # Extract message from sign\n",
        "    msg_bits = []\n",
        "    for i in range(message_length * 8):\n",
        "        sign = 1 if non_critical[i] >= 0 else -1\n",
        "        bit = 1 if sign > 0 else 0\n",
        "        msg_bits.append(bit)\n",
        "\n",
        "    # Convert binary to string\n",
        "    return msg_bits\n",
        "    # chars = [chr(int(bin_str[i:i+8], 2)) for i in range(0, len(bin_str), 8)]\n",
        "\n",
        "  def detect(self, audio):\n",
        "    \"\"\"Compute detection probability using steganalysis network\"\"\"\n",
        "    if not self.steganalysis_net:\n",
        "      raise ValueError(\"Steganalysis network not initialized\")\n",
        "\n",
        "    # Convert to tensor\n",
        "    audio_tensor = torch.as_tensor(audio, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      prob = self.steganalysis_net(audio_tensor).item()\n",
        "    return prob"
      ],
      "metadata": {
        "id": "bc5xRFmqKK_m"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Spread Spectrum"
      ],
      "metadata": {
        "id": "hmFr72llR9ks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SpreadSpectrum(EmbeddingModule):\n",
        "  def __init__(self):\n",
        "    \"\"\"\n",
        "    Initialize the Spread Spectrum steganography system\n",
        "\n",
        "    Parameters:\n",
        "      carrier_freq (int): Carrier frequency in Hz for embedding\n",
        "      chip_rate (int): How many samples per bit (spreading factor)\n",
        "      snr (int): Desired Signal to noise ratio in dB for embedding\n",
        "    \"\"\"\n",
        "    # Parameters for Gold sequence generation (example values)\n",
        "    self.taps1 = [5, 2]\n",
        "    self.taps2 = [5, 4, 2, 1] # Must be a preferred pair with taps1\n",
        "    self.seed1 = 0b11111\n",
        "    self.seed2 = 0b10101\n",
        "\n",
        "  def set_parameters(self, action):\n",
        "    \"\"\"\n",
        "    Parameters(action):\n",
        "      carrier_freq (int): Carrier frequency in Hz for embedding\n",
        "      chip_rate (int): How many samples per bit (spreading factor)\n",
        "      snr (int): Desired Signal to noise ratio in dB for embedding\n",
        "    \"\"\"\n",
        "    self.carrier_freq, self.chip_rate, self.snr_db = action;\n",
        "    self.chip_rate = int(self.chip_rate)\n",
        "\n",
        "  def _generate_m_sequence(self, taps, length, initial_state):\n",
        "      \"\"\"Generate an m-sequence.\"\"\"\n",
        "      lfsr = initial_state\n",
        "      seq = np.zeros(length)\n",
        "      # mask = sum(1 << (t - 1) for t in taps) # Mask is not used in this implementation\n",
        "\n",
        "      for i in range(length):\n",
        "          seq[i] = 1 if (lfsr & 1) else -1  # Convert to bipolar (-1, 1)\n",
        "          feedback = 0\n",
        "          for tap in taps:\n",
        "              feedback ^= (lfsr >> (tap - 1)) & 1\n",
        "          # Adjust the shift based on the number of bits in the initial state\n",
        "          lfsr = (lfsr >> 1) | (feedback << (len(bin(initial_state)) - 3)) # Assuming initial_state is not 0\n",
        "\n",
        "      return seq\n",
        "\n",
        "  def _generate_spreading_code(self, length):\n",
        "      \"\"\"Generate a Gold sequence.\"\"\"\n",
        "      # The lengths of the m-sequences must be the same\n",
        "      # and derived from the same primitive polynomial.\n",
        "      # The taps define the primitive polynomials.\n",
        "      # The seeds are the initial states of the LFSRs.\n",
        "\n",
        "      m_seq1 = self._generate_m_sequence(self.taps1, length, self.seed1)\n",
        "      m_seq2 = self._generate_m_sequence(self.taps2, length, self.seed2)\n",
        "\n",
        "      # XOR the two m-sequences\n",
        "      gold_seq = m_seq1 * m_seq2 # For bipolar sequences, XOR is multiplication\n",
        "\n",
        "      return gold_seq\n",
        "\n",
        "\n",
        "  def _calculate_required_bandwidth(self, message_length):\n",
        "    \"\"\"Calculate the required bandwidth based on message length.\"\"\"\n",
        "    # each bit is spread over chip_rate samples\n",
        "    return message_length * self.chip_rate\n",
        "\n",
        "  def embed(self, original_audio, msg_bits: np.ndarray, **kwargs):\n",
        "    \"\"\"\n",
        "    Embed a message into an audio file.\n",
        "\n",
        "    Parameters:\n",
        "      original_audio (str): Path to the audio file.\n",
        "      message (str): The message to be embedded.\n",
        "      output_path (str): Path to save the embedded audio file.\n",
        "    \"\"\"\n",
        "    msg_bits = msg_bits * 2 - 1 # convert to bipoloar (-1, 1)\n",
        "\n",
        "    # generate spreading codes\n",
        "    code_length  = len(msg_bits) * self.chip_rate\n",
        "    spreading_code = self._generate_spreading_code(code_length)\n",
        "\n",
        "    # create the spread message signal\n",
        "    spread_message = np.repeat(msg_bits, self.chip_rate) * spreading_code\n",
        "\n",
        "    # create carrier signal (sine wave at carrier frequency)\n",
        "    t = np.arange(len(spread_message)) / cfg.SAMPLE_RATE\n",
        "    carrier = np.sin(2 * np.pi * self.carrier_freq * t)\n",
        "\n",
        "    # modulate the message onto the carrier\n",
        "    modulated = spread_message * carrier\n",
        "\n",
        "    # adjust the signal power based on desired snr\n",
        "    signal_power = np.var(original_audio)\n",
        "    message_power = np.var(modulated)\n",
        "    desired_message_power = signal_power/ (10 ** (self.snr_db / 10))\n",
        "    scaling_factor = np.sqrt(desired_message_power / message_power)\n",
        "    modulated = modulated * scaling_factor\n",
        "\n",
        "    # pad or truncate the modulated signal to match audio length\n",
        "    if len(modulated) < len(original_audio):\n",
        "      modulated = np.pad(modulated, (0, len(original_audio) - len(modulated)), 'constant')\n",
        "    else:\n",
        "      modulated = modulated[:len(original_audio)]\n",
        "\n",
        "    # embed the message into the audio\n",
        "    stego_audio = original_audio + modulated\n",
        "    return stego_audio\n",
        "\n",
        "  def extract(self, stego_audio, message_length, original_audio_path=None, **kwargs):\n",
        "    \"\"\"\n",
        "        Extract a hidden message from a stego audio file.\n",
        "\n",
        "        Parameters:\n",
        "            stego_audio (nd.ndarray): Path to the stego audio file\n",
        "            message_length (int): Length of the hidden message in bits\n",
        "            original_audio_path (str): Optional path to original audio for comparison\n",
        "        \"\"\"\n",
        "    # if original audio is provided, subtract it to get just the message\n",
        "    if original_audio_path:\n",
        "      y_original, _ = librosa.load(original_audio_path, sr=cfg.SAMPLE_RATE)\n",
        "      y_diff = stego_audio - y_original\n",
        "    else:\n",
        "      y_diff = stego_audio\n",
        "\n",
        "    # generate the same spreading code used in embedding\n",
        "    code_length = message_length * 8 * self.chip_rate # 8 bits per character\n",
        "    # spreading_code = self._generate_spreading_code(code_length, seed=42) # Old simplified code\n",
        "    spreading_code = self._generate_spreading_code(code_length)\n",
        "\n",
        "\n",
        "    # create carrier signal\n",
        "    t = np.arange(len(spreading_code)) / cfg.SAMPLE_RATE\n",
        "    carrier = np.sin(2 * np.pi * self.carrier_freq * t)\n",
        "\n",
        "    # pad or truncate the carrier to match the difference signal\n",
        "    if len(carrier) < len(y_diff):\n",
        "      carrier = np.pad(carrier, (0, len(y_diff) - len(carrier)), 'constant')\n",
        "    else:\n",
        "      carrier = carrier[:len(y_diff)]\n",
        "\n",
        "    # demodulate the signal\n",
        "    demodulated = y_diff * carrier\n",
        "\n",
        "    # correlate with spreading code to extract bits\n",
        "    extracted_bits: list[int] = []\n",
        "    for i in range(message_length * 8):\n",
        "      start = i * self.chip_rate\n",
        "      end = start + self.chip_rate\n",
        "      if end > len(demodulated):\n",
        "        break\n",
        "      segment = demodulated[start:end]\n",
        "      code_segment = spreading_code[start:end]\n",
        "\n",
        "      # calculate correlation\n",
        "      correlation = np.sum(segment * code_segment)\n",
        "      extracted_bits.append(1 if correlation > 0 else 0)\n",
        "\n",
        "    # convert bits to string\n",
        "    return extracted_bits"
      ],
      "metadata": {
        "id": "Iv0zqtkMSArs"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --- MODULE 2: RL ACTOR-CRITIC NETWORKS ---"
      ],
      "metadata": {
        "id": "wfre1MB1Julg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PolicyNetwork(nn.Module):\n",
        "  \"\"\"Actor Network: Decides on the modification scale.\"\"\"\n",
        "  def __init__(self, input_dim, hidden_dim=256):\n",
        "    super(PolicyNetwork, self).__init__()\n",
        "    self.fc = nn.Sequential(\n",
        "        nn.Linear(input_dim, hidden_dim),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_dim, hidden_dim),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "    self.mean_layer = nn.Linear(hidden_dim, 1)\n",
        "    self.log_std_layer = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    features = self.fc(x)\n",
        "    mean = self.mean_layer(features)\n",
        "    log_std = self.log_std_layer(features)\n",
        "    log_std = torch.clamp(log_std, min=-20, max=2)  # Constrain for stability\n",
        "    return mean, log_std\n",
        "\n",
        "  def sample_action(self, state):\n",
        "    \"\"\"Sample action from policy distribution\"\"\"\n",
        "    state_tensor = torch.as_tensor(state, dtype=torch.float32).to(device)\n",
        "    mean, log_std = self.forward(state_tensor)\n",
        "    std = torch.exp(log_std)\n",
        "    normal_dist = torch.distributions.Normal(mean, std)\n",
        "    action = normal_dist.sample()\n",
        "    return action.cpu().detach().numpy().flatten()\n",
        "\n",
        "# 7. Feature Extractor for PPO\n",
        "class CustomFeatureExtractor(BaseFeaturesExtractor):\n",
        "  def __init__(self, observation_space: spaces.Box, features_dim = 128):\n",
        "    super().__init__(observation_space, features_dim)\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(observation_space.shape[0], 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, features_dim),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\n",
        "  def forward(self, observations: torch.Tensor) -> torch.Tensor:\n",
        "    return self.net(observations)"
      ],
      "metadata": {
        "id": "UmJ4bvTvJeK2"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --- MODULE 3: ENVIRONMENT NETWORK (STEGANALYZER) ---"
      ],
      "metadata": {
        "id": "EH5KOS4wJw0g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SteganalysisCNN(nn.Module):\n",
        "  \"\"\"A 1D CNN that acts as a steganalysis tool.\"\"\"\n",
        "  def __init__(self, input_channels=1):\n",
        "    super(SteganalysisCNN, self).__init__()\n",
        "    self.conv = nn.Sequential(\n",
        "        nn.Conv1d(input_channels, 32, kernel_size=5, stride=2, padding=2),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool1d(kernel_size=2),\n",
        "        nn.Conv1d(32, 64, kernel_size=5, stride=2, padding=2),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool1d(kernel_size=2),\n",
        "        nn.Flatten()\n",
        "    )\n",
        "\n",
        "    # Calculate output dimension after convolutions\n",
        "    test_input = torch.randn(1, input_channels, 500)\n",
        "    conv_output_dim = self.conv(test_input).shape[-1]\n",
        "\n",
        "    self.fc = nn.Sequential(\n",
        "        nn.Linear(conv_output_dim, 64),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(64, 1),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv(x)\n",
        "    return self.fc(x)\n",
        "\n",
        " #5. Custom Gym Environment for RL Training\n",
        "class AudioStegoEnv(gym.Env):\n",
        "  def __init__(self, audio_path, msg_bits: np.ndarray, method=\"sign-encoding\"):\n",
        "    super(AudioStegoEnv, self).__init__()\n",
        "\n",
        "    # Initialize audio and message\n",
        "    self.preprocessor = AudioPreprocessor(audio_path=audio_path)\n",
        "    self.original_audio = self.preprocessor.audio.copy()\n",
        "    self.msg_bits = msg_bits\n",
        "\n",
        "    self.action_space = spaces.Box(low=0.0, high=0.1, shape=(1,), dtype=np.float32)\n",
        "\n",
        "    # Define the action space as a Tuple of Box and Discrete spaces\n",
        "    if method == \"spread-spectrum\":\n",
        "      self.action_space = spaces.Box(low=np.array([5000, 50, 10], dtype=np.float32), high=np.array([15000, 200, 30], dtype=np.float32), dtype=np.float32) # continuous actions\n",
        "\n",
        "    self.observation_space = spaces.Box(low=-1.0, high=1.0,\n",
        "                                        shape=(N_MELS,), dtype=np.float32)\n",
        "    self.embedder: EmbeddingModule = SignEncoding() if method == \"sign-encoding\" else SpreadSpectrum()\n",
        "\n",
        "    # Initialize state\n",
        "    self.reset()\n",
        "\n",
        "  def reset(self, seed=None, options=None):\n",
        "    \"\"\"Reset environment to initial state\"\"\"\n",
        "    super().reset(seed=seed) # Call the parent class reset with seed\n",
        "    self.message_length = len(self.msg_bits) // 8  # Store message length in characters\n",
        "\n",
        "    self.current_audio = self.original_audio.copy()\n",
        "    self.magnitudes, self.phases = self.preprocessor.compute_mdct() # move\n",
        "    # Compute initial features\n",
        "    self.mask = self.preprocessor.get_non_critical_coeffs(self.magnitudes)\n",
        "    self.current_step = 0\n",
        "    return self._get_obs(), {} # Return observation and info dictionary\n",
        "\n",
        "  def _get_obs(self):\n",
        "    \"\"\"Extract MFCC features as observation\"\"\"\n",
        "    mfcc = librosa.feature.mfcc(y=self.current_audio, sr=SAMPLE_RATE,\n",
        "                                n_mfcc=N_MELS, n_fft=FRAME_SIZE,\n",
        "                                hop_length=HOP_LENGTH)\n",
        "    return np.mean(mfcc, axis=1)\n",
        "\n",
        "  def step(self, action):\n",
        "    \"\"\"Execute one embedding step\"\"\"\n",
        "    self.embedder.set_parameters(action)\n",
        "\n",
        "    # Embed message\n",
        "    self.current_audio = self.embedder.embed(magnitudes = self.magnitudes,phases=self.phases, mask = self.mask, msg_bits = self.msg_bits, original_audio=self.original_audio)\n",
        "\n",
        "    # Compute rewards\n",
        "    snr = self._calculate_snr()\n",
        "    psnr = self._calculate_psnr()\n",
        "    detection_prob = self._simulate_detection()\n",
        "    accuracy, ber = self._calculate_accuracy_ber()\n",
        "    reward = self._calculate_reward(snr,psnr, detection_prob, accuracy)\n",
        "\n",
        "    # Update state\n",
        "    self.current_step += 1\n",
        "    done = self.current_step >= 10  # Train for 10 steps\n",
        "    info = {\n",
        "        \"snr\": snr,\n",
        "        \"psnr\": psnr,\n",
        "        \"ber\": ber,\n",
        "        \"detection_prob\": detection_prob,\n",
        "        \"extraction_accuracy\": accuracy,\n",
        "        \"reward\": reward,\n",
        "        \"action\": action\n",
        "    }\n",
        "    return self._get_obs(), reward, done,False, info\n",
        "\n",
        "  def _calculate_snr(self):\n",
        "    \"\"\"Calculate Signal-to-Noise Ratio\"\"\"\n",
        "    # Ensure both audio arrays have the same length before calculating noise\n",
        "    min_len = min(len(self.current_audio), len(self.original_audio))\n",
        "    current_audio_trimmed = self.current_audio[:min_len]\n",
        "    original_audio_trimmed = self.original_audio[:min_len]\n",
        "\n",
        "    noise = current_audio_trimmed - original_audio_trimmed\n",
        "    signal_power = np.mean(original_audio_trimmed ** 2)\n",
        "    noise_power = np.mean(noise ** 2)\n",
        "\n",
        "    if noise_power == 0:\n",
        "        return 100  # High SNR if no noise\n",
        "\n",
        "    return 10 * np.log10(signal_power / noise_power)\n",
        "\n",
        "  def _calculate_psnr(self):\n",
        "    if len(self.current_audio) < len(self.original_audio):\n",
        "        stego_audio_padded = np.pad(self.current_audio, (0, len(self.original_audio) - len(self.current_audio)), 'constant')\n",
        "    else:\n",
        "        stego_audio_padded = self.current_audio[:len(self.original_audio)]\n",
        "\n",
        "    psnr = 10 * np.log10(np.max(self.original_audio ** 2) / np.mean((self.original_audio - stego_audio_padded) ** 2)) if np.mean((self.original_audio - stego_audio_padded) ** 2) > 0 else float('inf')\n",
        "    return psnr\n",
        "\n",
        "\n",
        "  def _simulate_detection(self):\n",
        "    \"\"\"Simulate steganalysis detection\"\"\"\n",
        "    # not real\n",
        "    snr = self._calculate_snr()\n",
        "    return 1 / (1 + np.exp(0.5 * (snr - 50)))  # Logistic function\n",
        "\n",
        "  def _calculate_accuracy_ber(self):\n",
        "    \"\"\"Calculate the accuracy of extracted bits\"\"\"\n",
        "    # Use the same mask as embedding for extraction simulation\n",
        "    extracted_bits = self.embedder.extract(stego_audio =self.current_audio,magnitudes=self.magnitudes, mask=self.mask,message_length= self.message_length)\n",
        "\n",
        "    min_len_bits = min(len(extracted_bits), len(self.msg_bits))\n",
        "    ber = np.mean([self.msg_bits[i] != extracted_bits[i] for i in range(min_len_bits)]) if min_len_bits > 0 else 1.0 # BER is 1 if no bits to compare\n",
        "    return 1.0 - ber, ber\n",
        "\n",
        "  def _calculate_reward(self, snr,psnr, detection_prob, extraction_accuracy):\n",
        "    \"\"\"Calculate reward balancing SNR, detectability, and extraction accuracy\"\"\"\n",
        "    # Target: SNR > 50 dB, detection_prob < 0.1, extraction_accuracy close to 1.0\n",
        "    snr_reward = min(snr / 50, 1.0)\n",
        "    detect_reward = 1.0 - min(detection_prob, 1.0)\n",
        "\n",
        "    # Weighted combination\n",
        "    return (\n",
        "        cfg.SNR_WEIGHTS * snr_reward +\n",
        "        cfg.DETECTION_WEIGHT * detect_reward +\n",
        "        cfg.EXTRACTION_ACCURACY_WEIGHT * extraction_accuracy\n",
        "    )"
      ],
      "metadata": {
        "id": "Vk-lzxwRJt0P"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# -- MAIN FRAMEWORK --"
      ],
      "metadata": {
        "id": "A69oR2apRBwA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RLAudioSteganography:\n",
        "  \"\"\"Main framework class\"\"\"\n",
        "  def __init__(self, cfg: Config) -> None:\n",
        "    self.cfg = cfg\n",
        "    self.embedded_mask = None # Store the mask used during embedding\n",
        "    self.original_magnitudes = None # Store original magnitudes\n",
        "\n",
        "  def Initialize_components(self, audio_path, method=\"sign-encoding\"):\n",
        "    \"\"\"Initialize components\"\"\"\n",
        "    self.preprocessor = AudioPreprocessor(audio_path=audio_path)\n",
        "    self.original_magnitudes, self.phases = self.preprocessor.compute_mdct()\n",
        "    self.mask = self.preprocessor.get_non_critical_coeffs(self.original_magnitudes)\n",
        "    self.steganalysis_net = SteganalysisCNN()\n",
        "    # self.steganalysis_net = SteganalysisCNN(input_channels=len(self.preprocessor.audio))\n",
        "    self.steganalysis_net.to(device)\n",
        "    self.method = method\n",
        "    self.embedder: EmbeddingModule = SignEncoding() if method == \"sign-encoding\" else SpreadSpectrum()\n",
        "\n",
        "  def string_to_bits(self, message):\n",
        "    \"\"\"Convert a string to a sequence of bits.\"\"\"\n",
        "    return np.array([int(bit) for bit in ''.join(format(ord(char), '08b') for char in message)], dtype=np.float32)\n",
        "\n",
        "  def bits_to_string(self, bits):\n",
        "    \"\"\"Convert a sequence of bits to a string.\"\"\"\n",
        "    text = ''\n",
        "    for bit in range(0, len(bits), 8):\n",
        "      if bit + 8 <= len(bits):\n",
        "        byte = ''.join(str(bit) for bit in bits[bit:bit + 8])\n",
        "        text += chr(int(byte, 2))\n",
        "    return text\n",
        "\n",
        "  def train_ppo(self, audio_path, message, total_timesteps=10000)-> PPO:\n",
        "    \"\"\"Train PPO agent for audio steganography\"\"\"\n",
        "    env = make_vec_env(lambda: AudioStegoEnv(audio_path, self.string_to_bits(message), method=self.method), n_envs=4)\n",
        "\n",
        "    policy_kwargs = dict(\n",
        "        features_extractor_class=CustomFeatureExtractor,\n",
        "        features_extractor_kwargs=dict(features_dim=128),\n",
        "    )\n",
        "\n",
        "    model = PPO(\"MlpPolicy\", env, policy_kwargs=policy_kwargs, verbose=1,\n",
        "                device=device, learning_rate=cfg.LEARNING_RATE_ACTOR, n_steps=cfg.EPISODES, batch_size=cfg.BATCH_SIZE)\n",
        "    # Instantiate the custom callback\n",
        "    custom_callback = CustomLoggingCallback()\n",
        "    model.learn(total_timesteps, custom_callback)\n",
        "\n",
        "    return model\n",
        "\n",
        "  def embed_message(self, audio_path, message, output_path, model):\n",
        "    \"\"\"Embed a message into an audio file using trained policy\"\"\"\n",
        "    # Re-initialize preprocessor and compute magnitudes/phases/mask for the specific audio being embedded into\n",
        "    self.preprocessor = AudioPreprocessor(audio_path=audio_path)\n",
        "    self.original_magnitudes, self.phases = self.preprocessor.compute_mdct()\n",
        "    self.mask = self.preprocessor.get_non_critical_coeffs(self.original_magnitudes)\n",
        "    self.embedded_mask = self.mask # Store the mask used for embedding\n",
        "\n",
        "    # Steganalysis network is initialized in Initialize_components, ensure it's done before embedding\n",
        "    # If embed_message is called standalone, ensure Initialize_components is called first with the correct audio_path\n",
        "\n",
        "    msg_bits = self.string_to_bits(message)\n",
        "    env = AudioStegoEnv(audio_path, msg_bits) # Use the correct audio_path and message\n",
        "    obs, info = env.reset()\n",
        "    action, _ = model.predict(obs, deterministic=True)\n",
        "    self.embedder.set_parameters(action)\n",
        "    # Convert message string to bits for embedding\n",
        "    message_bits = self.string_to_bits(message)\n",
        "    # stego_audio = self.embedder.embed(self.original_magnitudes, self.embedded_mask, message_bits)\n",
        "    stego_audio = self.embedder.embed(\n",
        "        magnitudes = self.original_magnitudes,\n",
        "        phases=self.phases,\n",
        "        mask = self.mask,\n",
        "        msg_bits = msg_bits,\n",
        "        original_audio=self.preprocessor.audio.copy())\n",
        "    self.preprocessor.save_audio(stego_audio,SAMPLE_RATE, output_path)\n",
        "    print(f\"Stego audio saved to {output_path}\")\n",
        "\n",
        "\n",
        "  def extract_message(self, stego_audio_path, msg_length):\n",
        "    \"\"\"Extract a message from a stego audio file\"\"\"\n",
        "    # Load the stego audio\n",
        "    preprocessor_stego = AudioPreprocessor(audio_path=stego_audio_path)\n",
        "    magnitudes_stego, phases_stego = preprocessor_stego.compute_mdct()\n",
        "\n",
        "    # Use the stored mask from embedding for extraction\n",
        "    if self.embedded_mask is None:\n",
        "        raise ValueError(\"Embedding must be performed before extraction to get the mask.\")\n",
        "\n",
        "    # extracted_message = self.embedder.extract(magnitudes_stego, self.embedded_mask, msg_length)\n",
        "    extracted_bits = self.embedder.extract(\n",
        "        stego_audio =preprocessor_stego.audio.copy(),\n",
        "        magnitudes=magnitudes_stego,\n",
        "        mask=self.mask,\n",
        "        message_length= msg_length)\n",
        "    return self.bits_to_string(extracted_bits)\n",
        "\n",
        "  def plot_training_history(self):\n",
        "    \"\"\"Plot training metrics\"\"\"\n",
        "    # plt.figure(figsize=(12, 8))\n",
        "    # plt.subplot(2, 2, 1)\n",
        "    # plt.plot(self.training_history['rewards'], label='Reward')\n",
        "    # plt.title('Training Rewards')\n",
        "    # plt.xlabel('Episode')\n",
        "    # plt.ylabel('Average Reward')\n",
        "    # plt.legend()\n",
        "    # plt.tight_layout()\n",
        "    # plt.show()"
      ],
      "metadata": {
        "id": "KWcvIP6yRIBc"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --- MAIN EXECUTION SCRIPT ---"
      ],
      "metadata": {
        "id": "ULjz6C-CKiMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Initialize the framework\n",
        "    cfg = Config()\n",
        "    framework = RLAudioSteganography(cfg)\n",
        "\n",
        "    # --- Training Phase ---\n",
        "    # Load sample audio for training\n",
        "    training_audio_path = librosa.ex('trumpet')\n",
        "    message_to_embed = \"There are things that we do not wish to know. and this is a secret I want to send to you okay\"\n",
        "\n",
        "    # Initialize components with the training audio\n",
        "    framework.Initialize_components(training_audio_path, method=\"spread-spectrum\")\n",
        "\n",
        "    # Train PPO agent\n",
        "    print(\"Training PPO agent...\")\n",
        "    model = framework.train_ppo(training_audio_path, message_to_embed, total_timesteps=10000)\n",
        "\n",
        "    # Save the trained model\n",
        "    model_save_path = \"ppo_audio_stego_model\"\n",
        "    model.save(model_save_path)\n",
        "    print(f\"Trained model saved to {model_save_path}\")\n",
        "\n",
        "    # --- Embedding with the Trained Model on a New Audio ---\n",
        "    print(\"\\nEmbedding message in a new audio file using the trained model...\")\n",
        "\n",
        "    # Load the saved model\n",
        "    loaded_model = PPO.load(model_save_path)\n",
        "    print(f\"Loaded model from {model_save_path}\")\n",
        "\n",
        "    # Define a new audio file and output path\n",
        "    new_audio_path = librosa.ex('trumpet', hq=True) # Using a different trumpet example\n",
        "    new_output_path = \"stego_new_audio.wav\"\n",
        "\n",
        "    # Initialize components with the *new* audio\n",
        "    framework.Initialize_components(new_audio_path, method=\"spread-spectrum\")\n",
        "\n",
        "    # Embed message using the loaded model\n",
        "    framework.embed_message(new_audio_path, message_to_embed, new_output_path, loaded_model)\n",
        "\n",
        "    # Extract message from the new stego audio\n",
        "    extracted_message = framework.extract_message(new_output_path, len(message_to_embed))\n",
        "    print(f\"\\nOriginal Message: {message_to_embed}\")\n",
        "    print(f\"Extracted Message from new audio: {extracted_message}\")\n",
        "\n",
        "    # --- Evaluation (Optional) ---\n",
        "    # You can add evaluation steps here similar to the commented out code\n",
        "    # to check SNR and detection probability on the new stego audio."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srWhvcBg59Qv",
        "outputId": "558f1e92-766e-41d0-fbaf-8acb9c7d3a76"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training PPO agent...\n",
            "Using cuda device\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------------------\n",
            "| rollout/                  |                     |\n",
            "|    ep_action              | [5000.   50.   10.] |\n",
            "|    ep_ber                 | 0.047               |\n",
            "|    ep_detection_prob      | 1                   |\n",
            "|    ep_extraction_accuracy | 0.953               |\n",
            "|    ep_len_mean            | 10                  |\n",
            "|    ep_rew_mean            | 2.92                |\n",
            "|    ep_reward              | 0.292               |\n",
            "|    ep_snr                 | 34                  |\n",
            "| time/                     |                     |\n",
            "|    fps                    | 5                   |\n",
            "|    iterations             | 1                   |\n",
            "|    time_elapsed           | 715                 |\n",
            "|    total_timesteps        | 4000                |\n",
            "---------------------------------------------------\n",
            "---------------------------------------------------\n",
            "| rollout/                  |                     |\n",
            "|    ep_action              | [5000.   50.   10.] |\n",
            "|    ep_ber                 | 0.047               |\n",
            "|    ep_detection_prob      | 1                   |\n",
            "|    ep_extraction_accuracy | 0.953               |\n",
            "|    ep_len_mean            | 10                  |\n",
            "|    ep_rew_mean            | 2.92                |\n",
            "|    ep_reward              | 0.292               |\n",
            "|    ep_snr                 | 34                  |\n",
            "| time/                     |                     |\n",
            "|    fps                    | 5                   |\n",
            "|    iterations             | 2                   |\n",
            "|    time_elapsed           | 1436                |\n",
            "|    total_timesteps        | 8000                |\n",
            "| train/                    |                     |\n",
            "|    approx_kl              | 0.0030158139        |\n",
            "|    clip_fraction          | 0.019               |\n",
            "|    clip_range             | 0.2                 |\n",
            "|    entropy_loss           | -4.25               |\n",
            "|    explained_variance     | 0.0118              |\n",
            "|    learning_rate          | 0.0003              |\n",
            "|    loss                   | 0.109               |\n",
            "|    n_updates              | 10                  |\n",
            "|    policy_gradient_loss   | 0.000917            |\n",
            "|    std                    | 0.995               |\n",
            "|    value_loss             | 0.249               |\n",
            "---------------------------------------------------\n",
            "---------------------------------------------------\n",
            "| rollout/                  |                     |\n",
            "|    ep_action              | [5000.   50.   10.] |\n",
            "|    ep_ber                 | 0.047               |\n",
            "|    ep_detection_prob      | 1                   |\n",
            "|    ep_extraction_accuracy | 0.953               |\n",
            "|    ep_len_mean            | 10                  |\n",
            "|    ep_rew_mean            | 2.92                |\n",
            "|    ep_reward              | 0.292               |\n",
            "|    ep_snr                 | 34                  |\n",
            "| time/                     |                     |\n",
            "|    fps                    | 5                   |\n",
            "|    iterations             | 3                   |\n",
            "|    time_elapsed           | 2150                |\n",
            "|    total_timesteps        | 12000               |\n",
            "| train/                    |                     |\n",
            "|    approx_kl              | 0.0040084757        |\n",
            "|    clip_fraction          | 0.0222              |\n",
            "|    clip_range             | 0.2                 |\n",
            "|    entropy_loss           | -4.23               |\n",
            "|    explained_variance     | 1.19e-07            |\n",
            "|    learning_rate          | 0.0003              |\n",
            "|    loss                   | 0.295               |\n",
            "|    n_updates              | 20                  |\n",
            "|    policy_gradient_loss   | -0.00139            |\n",
            "|    std                    | 0.988               |\n",
            "|    value_loss             | 0.577               |\n",
            "---------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
            "  warnings.warn(\n",
            "Downloading file 'sorohanro_-_solo-trumpet-06.hq.ogg' from 'https://librosa.org/data/audio/sorohanro_-_solo-trumpet-06.hq.ogg' to '/root/.cache/librosa'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trained model saved to ppo_audio_stego_model\n",
            "\n",
            "Embedding message in a new audio file using the trained model...\n",
            "Loaded model from ppo_audio_stego_model\n",
            "Stego audio saved to stego_new_audio.wav\n",
            "\n",
            "Original Message: There are things that we do not wish to know. and this is a secret I want to send to you okay\n",
            "Extracted Message from new audio: Thrc\u0000bs\u0005\u0000thbNsw pHat e(do0fot wuch to know. and 4hhs(is a(secretI(want to snd to you okay\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = Config()\n",
        "framework_test = RLAudioSteganography(cfg)\n",
        "\n",
        "audio = simple_audio_files[0]\n",
        "message = \"\"\"\n",
        "    Embed a message into an audio file.\n",
        "\n",
        "    Parameters:\n",
        "      audio_path (str): Path to the audio file.\n",
        "      message (str): The message to be embedded.\n",
        "      output_path (str): Path to save the embedded audio file.\n",
        "    \"\"\"\n",
        "output_path = \"test.wav\"\n",
        "\n",
        "framework_test.Initialize_components(audio_path=simple_audio_files[0], method=\"spread-spectrum\")\n",
        "# Load the saved model\n",
        "model_test = PPO.load(model_save_path)\n",
        "print(f\"Loaded model from {model_save_path}\")\n",
        "# Embed message using the loaded model\n",
        "framework_test.embed_message(audio, message, output_path, model_test)\n",
        "\n",
        "# Extract message from the new stego audio\n",
        "extracted_message = framework_test.extract_message(output_path, len(message))\n",
        "print(f\"\\nOriginal Message: {message}\")\n",
        "print(f\"Extracted Message from new audio: {extracted_message}\")"
      ],
      "metadata": {
        "id": "eH9P_-wMHSiN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "609e9f1a-60f6-4554-ed87-5726d84cf351"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded model from ppo_audio_stego_model\n",
            "Stego audio saved to test.wav\n",
            "\n",
            "Original Message: \n",
            "    Embed a message into an audio file.\n",
            "\n",
            "    Parameters:\n",
            "      audio_path (str): Path to the audio file.\n",
            "      message (str): The message to be embedded.\n",
            "      output_path (str): Path to save the embedded audio file.\n",
            "    \n",
            "Extracted Message from new audio: \n",
            "    Embed a message into an audi file.\b* (\u0000 Parametes:\n",
            "      audio[path (sr): Path tk the audio file,\n",
            "      meswagd (str): The message to bm embedded.\n",
            "      o_tput_path (str): P`th to savebthe embedded audio file.\n",
            "    \n"
          ]
        }
      ]
    }
  ]
}